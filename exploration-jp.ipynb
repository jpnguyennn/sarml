{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07ba54c",
   "metadata": {},
   "source": [
    "# Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7432b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# baseline model libraries\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# first model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# second model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# third model\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# fourth model\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# ensemble model\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8b19b",
   "metadata": {},
   "source": [
    "# Exploring the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77d1111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (21464, 2)\n",
      "Valid Shape: (716, 2)\n",
      "Test Shape: (966, 2)\n",
      "----------\n",
      "                                                text  label\n",
      "0  states slow to shut down weak teacher educatio...      0\n",
      "1    drone places fresh kill on steps of white house      1\n",
      "2  report: majority of instances of people gettin...      1\n",
      "3  sole remaining lung filled with rich, satisfyi...      1\n",
      "4                       the gop's stockholm syndrome      0\n",
      "----------\n",
      "label\n",
      "0    11248\n",
      "1    10216\n",
      "Name: count, dtype: int64\n",
      "----------\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# loading datasets\n",
    "train_df_main = pd.read_csv('./datasets/train.csv')\n",
    "valid_df_main = pd.read_csv('./datasets/valid.csv')\n",
    "test_df_main = pd.read_csv('./datasets/test.csv')\n",
    "\n",
    "# display shapes\n",
    "print(f\"Train Shape: {train_df_main.shape}\")\n",
    "print(f\"Valid Shape: {valid_df_main.shape}\")\n",
    "print(f\"Test Shape: {test_df_main.shape}\")\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "# preview training data\n",
    "print(train_df_main.head())\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "# check for class balance\n",
    "print(train_df_main['label'].value_counts())\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "# check for missing values\n",
    "print(train_df_main.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea92090",
   "metadata": {},
   "source": [
    "# Preprocessing all datasets\n",
    "\n",
    "Preprocessing includes splitting apart the sentences into tokens, lowercasing all words, and making sure there is no whitespace within the sentences themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac79f455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['states', 'slow', 'to', 'shut', 'down', 'weak', 'teacher', 'education', 'programs'], ['drone', 'places', 'fresh', 'kill', 'on', 'steps', 'of', 'white', 'house'], ['report:', 'majority', 'of', 'instances', 'of', 'people', 'getting', 'lives', 'back', 'on', 'track', 'occur', 'immediately', 'after', 'visit', 'to', 'buffalo', 'wild', 'wings'], ['sole', 'remaining', 'lung', 'filled', 'with', 'rich,', 'satisfying', 'flavor'], ['the', \"gop's\", 'stockholm', 'syndrome']]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# unprocessed text and labels\n",
    "X_train = train_df_main['text']\n",
    "X_valid = valid_df_main['text']\n",
    "X_test = test_df_main['text']\n",
    "\n",
    "y_train = train_df_main['label']\n",
    "y_valid = valid_df_main['label']\n",
    "y_test = test_df_main['label']\n",
    "\n",
    "# processed text\n",
    "X_train_processed = []\n",
    "X_valid_processed = []\n",
    "X_test_processed = []\n",
    "\n",
    "for t in X_train:\n",
    "    X_train_processed.append(preprocess(t))\n",
    "\n",
    "for t in X_valid:\n",
    "    X_valid_processed.append(preprocess(t))\n",
    "\n",
    "for t in X_test:\n",
    "    X_test_processed.append(preprocess(t))\n",
    "    \n",
    "# validating that preprocessing worked\n",
    "print(X_train_processed[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f6ac6",
   "metadata": {},
   "source": [
    "K-Fold Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1bc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86523190",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "logistic regression + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d51a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7663\n"
     ]
    }
   ],
   "source": [
    "# baseline model\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# the vectorizer uses the main dataframe\n",
    "X_train_baseline = vectorizer.fit_transform(X_train)\n",
    "X_valid_baseline = vectorizer.transform(X_valid)\n",
    "X_test_baseline = vectorizer.transform(X_test)\n",
    "\n",
    "baseline_model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "baseline_model.fit(X_train_baseline, y_train)\n",
    "\n",
    "valid_preds = baseline_model.predict(X_valid_baseline)\n",
    "\n",
    "print(f\"Baseline Validation Accuracy: {accuracy_score(y_valid, valid_preds):.4f}\")\n",
    "print(f\"Baseline Validation F1 Score: {f1_score(y_valid, valid_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ea68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- TESTING i = 1000 -----\n",
      "Baseline Validation Accuracy: 0.7332\n",
      "Baseline Validation F1 Score: 0.7170\n",
      "----- TESTING i = 1100 -----\n",
      "Baseline Validation Accuracy: 0.7249\n",
      "Baseline Validation F1 Score: 0.7099\n",
      "----- TESTING i = 1200 -----\n",
      "Baseline Validation Accuracy: 0.7207\n",
      "Baseline Validation F1 Score: 0.7041\n",
      "----- TESTING i = 1300 -----\n",
      "Baseline Validation Accuracy: 0.7193\n",
      "Baseline Validation F1 Score: 0.7022\n",
      "----- TESTING i = 1400 -----\n",
      "Baseline Validation Accuracy: 0.7263\n",
      "Baseline Validation F1 Score: 0.7126\n",
      "----- TESTING i = 1500 -----\n",
      "Baseline Validation Accuracy: 0.7277\n",
      "Baseline Validation F1 Score: 0.7128\n",
      "----- TESTING i = 1600 -----\n",
      "Baseline Validation Accuracy: 0.7277\n",
      "Baseline Validation F1 Score: 0.7178\n",
      "----- TESTING i = 1700 -----\n",
      "Baseline Validation Accuracy: 0.7332\n",
      "Baseline Validation F1 Score: 0.7236\n",
      "----- TESTING i = 1800 -----\n",
      "Baseline Validation Accuracy: 0.7332\n",
      "Baseline Validation F1 Score: 0.7236\n",
      "----- TESTING i = 1900 -----\n",
      "Baseline Validation Accuracy: 0.7388\n",
      "Baseline Validation F1 Score: 0.7294\n",
      "----- TESTING i = 2000 -----\n",
      "Baseline Validation Accuracy: 0.7472\n",
      "Baseline Validation F1 Score: 0.7381\n",
      "----- TESTING i = 2100 -----\n",
      "Baseline Validation Accuracy: 0.7444\n",
      "Baseline Validation F1 Score: 0.7336\n",
      "----- TESTING i = 2200 -----\n",
      "Baseline Validation Accuracy: 0.7444\n",
      "Baseline Validation F1 Score: 0.7359\n",
      "----- TESTING i = 2300 -----\n",
      "Baseline Validation Accuracy: 0.7444\n",
      "Baseline Validation F1 Score: 0.7359\n",
      "----- TESTING i = 2400 -----\n",
      "Baseline Validation Accuracy: 0.7472\n",
      "Baseline Validation F1 Score: 0.7373\n",
      "----- TESTING i = 2500 -----\n",
      "Baseline Validation Accuracy: 0.7556\n",
      "Baseline Validation F1 Score: 0.7453\n",
      "----- TESTING i = 2600 -----\n",
      "Baseline Validation Accuracy: 0.7556\n",
      "Baseline Validation F1 Score: 0.7445\n",
      "----- TESTING i = 2700 -----\n",
      "Baseline Validation Accuracy: 0.7542\n",
      "Baseline Validation F1 Score: 0.7434\n",
      "----- TESTING i = 2800 -----\n",
      "Baseline Validation Accuracy: 0.7528\n",
      "Baseline Validation F1 Score: 0.7431\n",
      "----- TESTING i = 2900 -----\n",
      "Baseline Validation Accuracy: 0.7500\n",
      "Baseline Validation F1 Score: 0.7402\n",
      "----- TESTING i = 3000 -----\n",
      "Baseline Validation Accuracy: 0.7556\n",
      "Baseline Validation F1 Score: 0.7460\n",
      "----- TESTING i = 3100 -----\n",
      "Baseline Validation Accuracy: 0.7556\n",
      "Baseline Validation F1 Score: 0.7453\n",
      "----- TESTING i = 3200 -----\n",
      "Baseline Validation Accuracy: 0.7570\n",
      "Baseline Validation F1 Score: 0.7464\n",
      "----- TESTING i = 3300 -----\n",
      "Baseline Validation Accuracy: 0.7598\n",
      "Baseline Validation F1 Score: 0.7500\n",
      "----- TESTING i = 3400 -----\n",
      "Baseline Validation Accuracy: 0.7626\n",
      "Baseline Validation F1 Score: 0.7529\n",
      "----- TESTING i = 3500 -----\n",
      "Baseline Validation Accuracy: 0.7640\n",
      "Baseline Validation F1 Score: 0.7554\n",
      "----- TESTING i = 3600 -----\n",
      "Baseline Validation Accuracy: 0.7668\n",
      "Baseline Validation F1 Score: 0.7583\n",
      "----- TESTING i = 3700 -----\n",
      "Baseline Validation Accuracy: 0.7682\n",
      "Baseline Validation F1 Score: 0.7608\n",
      "----- TESTING i = 3800 -----\n",
      "Baseline Validation Accuracy: 0.7682\n",
      "Baseline Validation F1 Score: 0.7608\n",
      "----- TESTING i = 3900 -----\n",
      "Baseline Validation Accuracy: 0.7709\n",
      "Baseline Validation F1 Score: 0.7644\n",
      "----- TESTING i = 4000 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7655\n",
      "----- TESTING i = 4100 -----\n",
      "Baseline Validation Accuracy: 0.7765\n",
      "Baseline Validation F1 Score: 0.7688\n",
      "----- TESTING i = 4200 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7652\n",
      "----- TESTING i = 4300 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7670\n",
      "----- TESTING i = 4400 -----\n",
      "Baseline Validation Accuracy: 0.7709\n",
      "Baseline Validation F1 Score: 0.7630\n",
      "----- TESTING i = 4500 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7677\n",
      "----- TESTING i = 4600 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7659\n",
      "----- TESTING i = 4700 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7652\n",
      "----- TESTING i = 4800 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7641\n",
      "----- TESTING i = 4900 -----\n",
      "Baseline Validation Accuracy: 0.7765\n",
      "Baseline Validation F1 Score: 0.7688\n",
      "----- TESTING i = 5000 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7663\n",
      "----- TESTING i = 5100 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7670\n",
      "----- TESTING i = 5200 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7699\n",
      "----- TESTING i = 5300 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7699\n",
      "----- TESTING i = 5400 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7663\n",
      "----- TESTING i = 5500 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7638\n",
      "----- TESTING i = 5600 -----\n",
      "Baseline Validation Accuracy: 0.7765\n",
      "Baseline Validation F1 Score: 0.7674\n",
      "----- TESTING i = 5700 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7656\n",
      "----- TESTING i = 5800 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7656\n",
      "----- TESTING i = 5900 -----\n",
      "Baseline Validation Accuracy: 0.7696\n",
      "Baseline Validation F1 Score: 0.7605\n",
      "----- TESTING i = 6000 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7659\n",
      "----- TESTING i = 6100 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7677\n",
      "----- TESTING i = 6200 -----\n",
      "Baseline Validation Accuracy: 0.7793\n",
      "Baseline Validation F1 Score: 0.7717\n",
      "----- TESTING i = 6300 -----\n",
      "Baseline Validation Accuracy: 0.7793\n",
      "Baseline Validation F1 Score: 0.7710\n",
      "----- TESTING i = 6400 -----\n",
      "Baseline Validation Accuracy: 0.7793\n",
      "Baseline Validation F1 Score: 0.7710\n",
      "----- TESTING i = 6500 -----\n",
      "Baseline Validation Accuracy: 0.7793\n",
      "Baseline Validation F1 Score: 0.7703\n",
      "----- TESTING i = 6600 -----\n",
      "Baseline Validation Accuracy: 0.7765\n",
      "Baseline Validation F1 Score: 0.7668\n",
      "----- TESTING i = 6700 -----\n",
      "Baseline Validation Accuracy: 0.7765\n",
      "Baseline Validation F1 Score: 0.7668\n",
      "----- TESTING i = 6800 -----\n",
      "Baseline Validation Accuracy: 0.7793\n",
      "Baseline Validation F1 Score: 0.7703\n",
      "----- TESTING i = 6900 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7686\n",
      "----- TESTING i = 7000 -----\n",
      "Baseline Validation Accuracy: 0.7682\n",
      "Baseline Validation F1 Score: 0.7566\n",
      "----- TESTING i = 7100 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7632\n",
      "----- TESTING i = 7200 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7679\n",
      "----- TESTING i = 7300 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7606\n",
      "----- TESTING i = 7400 -----\n",
      "Baseline Validation Accuracy: 0.7709\n",
      "Baseline Validation F1 Score: 0.7588\n",
      "----- TESTING i = 7500 -----\n",
      "Baseline Validation Accuracy: 0.7765\n",
      "Baseline Validation F1 Score: 0.7654\n",
      "----- TESTING i = 7600 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7665\n",
      "----- TESTING i = 7700 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7665\n",
      "----- TESTING i = 7800 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7672\n",
      "----- TESTING i = 7900 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7672\n",
      "----- TESTING i = 8000 -----\n",
      "Baseline Validation Accuracy: 0.7779\n",
      "Baseline Validation F1 Score: 0.7679\n",
      "----- TESTING i = 8100 -----\n",
      "Baseline Validation Accuracy: 0.7751\n",
      "Baseline Validation F1 Score: 0.7643\n",
      "----- TESTING i = 8200 -----\n",
      "Baseline Validation Accuracy: 0.7709\n",
      "Baseline Validation F1 Score: 0.7595\n",
      "----- TESTING i = 8300 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7618\n",
      "----- TESTING i = 8400 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7606\n",
      "----- TESTING i = 8500 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7625\n",
      "----- TESTING i = 8600 -----\n",
      "Baseline Validation Accuracy: 0.7737\n",
      "Baseline Validation F1 Score: 0.7625\n",
      "----- TESTING i = 8700 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7606\n",
      "----- TESTING i = 8800 -----\n",
      "Baseline Validation Accuracy: 0.7709\n",
      "Baseline Validation F1 Score: 0.7595\n",
      "----- TESTING i = 8900 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7613\n",
      "----- TESTING i = 9000 -----\n",
      "Baseline Validation Accuracy: 0.7682\n",
      "Baseline Validation F1 Score: 0.7559\n",
      "----- TESTING i = 9100 -----\n",
      "Baseline Validation Accuracy: 0.7668\n",
      "Baseline Validation F1 Score: 0.7533\n",
      "----- TESTING i = 9200 -----\n",
      "Baseline Validation Accuracy: 0.7682\n",
      "Baseline Validation F1 Score: 0.7552\n",
      "----- TESTING i = 9300 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7606\n",
      "----- TESTING i = 9400 -----\n",
      "Baseline Validation Accuracy: 0.7696\n",
      "Baseline Validation F1 Score: 0.7570\n",
      "----- TESTING i = 9500 -----\n",
      "Baseline Validation Accuracy: 0.7682\n",
      "Baseline Validation F1 Score: 0.7552\n",
      "----- TESTING i = 9600 -----\n",
      "Baseline Validation Accuracy: 0.7682\n",
      "Baseline Validation F1 Score: 0.7559\n",
      "----- TESTING i = 9700 -----\n",
      "Baseline Validation Accuracy: 0.7696\n",
      "Baseline Validation F1 Score: 0.7570\n",
      "----- TESTING i = 9800 -----\n",
      "Baseline Validation Accuracy: 0.7723\n",
      "Baseline Validation F1 Score: 0.7606\n",
      "----- TESTING i = 9900 -----\n",
      "Baseline Validation Accuracy: 0.7709\n",
      "Baseline Validation F1 Score: 0.7588\n"
     ]
    }
   ],
   "source": [
    "# pinpointing best feature value for vectorizer\n",
    "best_acc = 0\n",
    "best_max_features = 1000\n",
    "for i in range(1000, 10000, 100):\n",
    "    temp_vectorizer = TfidfVectorizer(stop_words='english', max_features=i, ngram_range=(1, 2))\n",
    "    \n",
    "    X_train_baseline = temp_vectorizer.fit_transform(X_train)\n",
    "    X_valid_baseline = temp_vectorizer.transform(X_valid)\n",
    "    X_test_baseline = temp_vectorizer.transform(X_test)\n",
    "\n",
    "    baseline_model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "    baseline_model.fit(X_train_baseline, y_train)\n",
    "\n",
    "    valid_preds = baseline_model.predict(X_valid_baseline)\n",
    "\n",
    "    print(f\"----- TESTING i = {i} -----\")\n",
    "    print(f\"Baseline Validation Accuracy: {accuracy_score(y_valid, valid_preds):.4f}\")\n",
    "    print(f\"Baseline Validation F1 Score: {f1_score(y_valid, valid_preds):.4f}\")\n",
    "    \n",
    "    if accuracy_score(y_valid, valid_preds) > best_acc:\n",
    "        best_acc = accuracy_score(y_valid, valid_preds)\n",
    "        best_max_features = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68b3871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7793296089385475\n",
      "6200\n"
     ]
    }
   ],
   "source": [
    "print(best_acc)\n",
    "print(best_max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e644e2",
   "metadata": {},
   "source": [
    "# First Model Exploration\n",
    "\n",
    "SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad119ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.7682\n",
      "SVM Validation F1 Score: 0.7580\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "best_vectorizer = TfidfVectorizer(stop_words='english', max_features=5200, ngram_range=(1, 2))\n",
    "\n",
    "X_train_svm = best_vectorizer.fit_transform(X_train)\n",
    "X_valid_svm = best_vectorizer.transform(X_valid)\n",
    "\n",
    "# use linearsvc since it is better than the normal svc\n",
    "# we also use td-idf with svms\n",
    "svm_model = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm_model.fit(X_train_svm, y_train)\n",
    "\n",
    "valid_preds_svm = svm_model.predict(X_valid_svm)\n",
    "\n",
    "print(f\"SVM Validation Accuracy: {accuracy_score(y_valid, valid_preds_svm):.4f}\")\n",
    "print(f\"SVM Validation F1 Score: {f1_score(y_valid, valid_preds_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc_svm = 0\n",
    "best_max_features_svm = 1000\n",
    "for i in range(1000, 10000, 100):\n",
    "    temp_vectorizer = TfidfVectorizer(stop_words='english', max_features=i, ngram_range=(1, 2))\n",
    "    \n",
    "    X_train_svm = temp_vectorizer.fit_transform(X_train)\n",
    "    X_valid_svm = temp_vectorizer.transform(X_valid)\n",
    "\n",
    "    # use linearsvc since it is better than the normal svc\n",
    "    # we also use td-idf with svms\n",
    "    svm_model = LinearSVC(random_state=42, max_iter=10000)\n",
    "    svm_model.fit(X_train_svm, y_train)\n",
    "\n",
    "    valid_preds_svm = svm_model.predict(X_valid_svm)\n",
    "\n",
    "    print(f\"----- TESTING i = {i} -----\")\n",
    "    print(f\"SVM Validation Accuracy: {accuracy_score(y_valid, valid_preds_svm):.4f}\")\n",
    "    print(f\"SVM Validation F1 Score: {f1_score(y_valid, valid_preds_svm):.4f}\")\n",
    "    \n",
    "    if accuracy_score(y_valid, valid_preds_svm) > best_acc_svm:\n",
    "        best_acc_svm = accuracy_score(y_valid, valid_preds_svm)\n",
    "        best_max_features_svm = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a455541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7821229050279329\n",
      "3500\n"
     ]
    }
   ],
   "source": [
    "print(best_acc_svm)\n",
    "print(best_max_features_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512b1da",
   "metadata": {},
   "source": [
    "# Second Model Exploration\n",
    "\n",
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5acb8575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "336/336 [==============================] - 46s 104ms/step - loss: 0.4448 - accuracy: 0.7836 - val_loss: 0.3402 - val_accuracy: 0.8701\n",
      "Epoch 2/5\n",
      "336/336 [==============================] - 30s 91ms/step - loss: 0.2453 - accuracy: 0.9022 - val_loss: 0.3654 - val_accuracy: 0.8422\n",
      "Epoch 3/5\n",
      "336/336 [==============================] - 28s 83ms/step - loss: 0.1735 - accuracy: 0.9361 - val_loss: 0.4162 - val_accuracy: 0.8352\n",
      "Epoch 4/5\n",
      "336/336 [==============================] - 27s 81ms/step - loss: 0.1347 - accuracy: 0.9527 - val_loss: 0.4862 - val_accuracy: 0.8282\n",
      "Epoch 5/5\n",
      "336/336 [==============================] - 27s 79ms/step - loss: 0.1021 - accuracy: 0.9654 - val_loss: 0.5593 - val_accuracy: 0.8170\n",
      "23/23 [==============================] - 1s 24ms/step\n",
      "LSTM Validation Accuracy: 0.8170\n",
      "LSTM Validation F1 Score: 0.8137\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_df_main['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_df_main['text']), maxlen=100)\n",
    "X_valid_seq = pad_sequences(tokenizer.texts_to_sequences(valid_df_main['text']), maxlen=100)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df_main['text']), maxlen=100)\n",
    "\n",
    "# 2. Model: Define a simple LSTM network \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 4. Evaluation\n",
    "lstm_probs = model.predict(X_valid_seq)\n",
    "lstm_preds = (lstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Validation Accuracy: {accuracy_score(y_valid, lstm_preds):.4f}\")\n",
    "print(f\"LSTM Validation F1 Score: {f1_score(y_valid, lstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28549e41",
   "metadata": {},
   "source": [
    "# Third Model Exploration\n",
    "\n",
    "BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dee1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "336/336 [==============================] - 23s 59ms/step - loss: 0.4906 - accuracy: 0.7544 - val_loss: 0.3436 - val_accuracy: 0.8589\n",
      "Epoch 2/5\n",
      "336/336 [==============================] - 22s 64ms/step - loss: 0.2620 - accuracy: 0.8989 - val_loss: 0.3528 - val_accuracy: 0.8520\n",
      "Epoch 3/5\n",
      "336/336 [==============================] - 21s 63ms/step - loss: 0.1886 - accuracy: 0.9333 - val_loss: 0.3902 - val_accuracy: 0.8478\n",
      "Epoch 4/5\n",
      "336/336 [==============================] - 23s 67ms/step - loss: 0.1452 - accuracy: 0.9491 - val_loss: 0.4610 - val_accuracy: 0.8268\n",
      "Epoch 5/5\n",
      "336/336 [==============================] - 25s 74ms/step - loss: 0.1120 - accuracy: 0.9619 - val_loss: 0.5348 - val_accuracy: 0.8282\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "Bi-LSTM Validation Accuracy: 0.8282\n",
      "Bi-LSTM Validation F1 Score: 0.8172\n"
     ]
    }
   ],
   "source": [
    "# 1. Update Sequences: Use the 'clean_text' from the previous step\n",
    "# We re-fit the tokenizer on the cleaner text\n",
    "tokenizer_clean = Tokenizer(num_words=10000)\n",
    "tokenizer_clean.fit_on_texts(train_df_main['clean_text'])\n",
    "\n",
    "X_train_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(train_df_main['clean_text']), maxlen=100)\n",
    "X_valid_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(valid_df_main['clean_text']), maxlen=100)\n",
    "X_test_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(test_df_main['clean_text']), maxlen=100)\n",
    "\n",
    "# 2. Improved Model: Stacked Bi-LSTM\n",
    "# We stack two Bidirectional LSTM layers to learn more complex patterns\n",
    "bilstm_improved = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)), # Return sequences is required to stack another LSTM\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_improved.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training with Early Stopping\n",
    "# Stop training if validation loss doesn't improve for 3 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "bilstm_improved.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 4. Evaluation\n",
    "probs_bilstm_improved = bilstm_improved.predict(X_valid_seq_clean)\n",
    "preds_bilstm_improved = (probs_bilstm_improved > 0.5).astype(int)\n",
    "\n",
    "print(f\"Improved Bi-LSTM Accuracy: {accuracy_score(y_valid, preds_bilstm_improved):.4f}\")\n",
    "print(f\"Improved Bi-LSTM F1 Score: {f1_score(y_valid, preds_bilstm_improved):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790d0e6",
   "metadata": {},
   "source": [
    "# Fourth Model\n",
    "\n",
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model: 1D Convolutional Neural Network\n",
    "# - Conv1D with kernel_size=5 looks at 5-word windows to find sarcastic phrases\n",
    "# - GlobalMaxPooling1D keeps only the strongest signal found in the text\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same 'clean' sequences and early stopping as before\n",
    "cnn_model.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 3. Evaluation\n",
    "probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "preds_cnn = (probs_cnn > 0.5).astype(int)\n",
    "\n",
    "print(f\"CNN Validation Accuracy: {accuracy_score(y_valid, preds_cnn):.4f}\")\n",
    "print(f\"CNN Validation F1 Score: {f1_score(y_valid, preds_cnn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ae452",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "\n",
    "All of the models join together and create a beautiful new model to average out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27972ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FOLD 1/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7559 (0.0s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8428 (50.8s)\n",
      "[3/3] Training CNN... Done! F1: 0.8428 (50.8s)\n",
      "[3/3] Training CNN... Done! F1: 0.8416 (12.0s)\n",
      "\n",
      "Fold 1 completed in 62.9s\n",
      "  SVM: 0.7559 | BiLSTM: 0.8428 | CNN: 0.8416\n",
      "\n",
      "⏱️  Estimated time remaining: 4.2 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 2/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7515 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8416 (12.0s)\n",
      "\n",
      "Fold 1 completed in 62.9s\n",
      "  SVM: 0.7559 | BiLSTM: 0.8428 | CNN: 0.8416\n",
      "\n",
      "⏱️  Estimated time remaining: 4.2 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 2/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7515 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8440 (68.5s)\n",
      "[3/3] Training CNN... Done! F1: 0.8440 (68.5s)\n",
      "[3/3] Training CNN... Done! F1: 0.8530 (16.9s)\n",
      "\n",
      "Fold 2 completed in 85.6s\n",
      "  SVM: 0.7515 | BiLSTM: 0.8440 | CNN: 0.8530\n",
      "\n",
      "⏱️  Estimated time remaining: 3.7 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 3/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7706 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8530 (16.9s)\n",
      "\n",
      "Fold 2 completed in 85.6s\n",
      "  SVM: 0.7515 | BiLSTM: 0.8440 | CNN: 0.8530\n",
      "\n",
      "⏱️  Estimated time remaining: 3.7 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 3/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7706 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8542 (320.7s)\n",
      "[3/3] Training CNN... Done! F1: 0.8542 (320.7s)\n",
      "[3/3] Training CNN... Done! F1: 0.8460 (17.4s)\n",
      "\n",
      "Fold 3 completed in 338.1s\n",
      "  SVM: 0.7706 | BiLSTM: 0.8542 | CNN: 0.8460\n",
      "\n",
      "⏱️  Estimated time remaining: 5.4 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 4/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7525 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8460 (17.4s)\n",
      "\n",
      "Fold 3 completed in 338.1s\n",
      "  SVM: 0.7706 | BiLSTM: 0.8542 | CNN: 0.8460\n",
      "\n",
      "⏱️  Estimated time remaining: 5.4 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 4/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7525 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8464 (101.0s)\n",
      "[3/3] Training CNN... Done! F1: 0.8464 (101.0s)\n",
      "[3/3] Training CNN... Done! F1: 0.8489 (11.6s)\n",
      "\n",
      "Fold 4 completed in 112.7s\n",
      "  SVM: 0.7525 | BiLSTM: 0.8464 | CNN: 0.8489\n",
      "\n",
      "⏱️  Estimated time remaining: 2.5 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 5/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7665 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8489 (11.6s)\n",
      "\n",
      "Fold 4 completed in 112.7s\n",
      "  SVM: 0.7525 | BiLSTM: 0.8464 | CNN: 0.8489\n",
      "\n",
      "⏱️  Estimated time remaining: 2.5 minutes\n",
      "\n",
      "======================================================================\n",
      "FOLD 5/5\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training SVM... Done! F1: 0.7665 (0.1s)\n",
      "[2/3] Training BiLSTM... Done! F1: 0.8427 (88.5s)\n",
      "[3/3] Training CNN... Done! F1: 0.8427 (88.5s)\n",
      "[3/3] Training CNN... Done! F1: 0.8410 (12.1s)\n",
      "\n",
      "Fold 5 completed in 100.7s\n",
      "  SVM: 0.7665 | BiLSTM: 0.8427 | CNN: 0.8410\n",
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "\n",
      "Average F1 scores across all folds:\n",
      "  CNN       : 0.8461 (±0.0045)\n",
      "  BILSTM    : 0.8460 (±0.0043)\n",
      "  SVM       : 0.7594 (±0.0077)\n",
      "\n",
      "Calculated weights (based on F1 performance):\n",
      "  CNN       : 0.3451 (34.5%)\n",
      "  BILSTM    : 0.3451 (34.5%)\n",
      "  SVM       : 0.3098 (31.0%)\n",
      "\n",
      "Total training time: 11.7 minutes\n",
      "Average time per fold: 140.0 seconds\n",
      "Done! F1: 0.8410 (12.1s)\n",
      "\n",
      "Fold 5 completed in 100.7s\n",
      "  SVM: 0.7665 | BiLSTM: 0.8427 | CNN: 0.8410\n",
      "\n",
      "======================================================================\n",
      "RESULTS\n",
      "======================================================================\n",
      "\n",
      "Average F1 scores across all folds:\n",
      "  CNN       : 0.8461 (±0.0045)\n",
      "  BILSTM    : 0.8460 (±0.0043)\n",
      "  SVM       : 0.7594 (±0.0077)\n",
      "\n",
      "Calculated weights (based on F1 performance):\n",
      "  CNN       : 0.3451 (34.5%)\n",
      "  BILSTM    : 0.3451 (34.5%)\n",
      "  SVM       : 0.3098 (31.0%)\n",
      "\n",
      "Total training time: 11.7 minutes\n",
      "Average time per fold: 140.0 seconds\n"
     ]
    }
   ],
   "source": [
    "X_combined = pd.concat([train_df_main['text'], valid_df_main['text']], ignore_index=True)\n",
    "y_combined = pd.concat([train_df_main['label'], valid_df_main['label']], ignore_index=True)\n",
    "\n",
    "# Change 1: Reduce folds from 7 to 5 (saves ~40% time)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "all_predictions = {\n",
    "    'svm': [], 'bilstm': [], 'cnn': []\n",
    "}\n",
    "all_labels = []\n",
    "\n",
    "model_f1_scores = {\n",
    "    'svm': [], 'bilstm': [], 'cnn': []\n",
    "}\n",
    "\n",
    "# Track timing\n",
    "fold_times = []\n",
    "# TF-IDF\n",
    "vectorizer_svm = TfidfVectorizer(stop_words='english', max_features=3500, ngram_range=(1,2))\n",
    "X_tfidf_all = vectorizer_svm.fit_transform(X_combined)\n",
    "\n",
    "# Tokenizer for Deep Learning\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_combined)\n",
    "X_seq_all = pad_sequences(tokenizer.texts_to_sequences(X_combined), maxlen=100)\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(kfold.split(X_combined, y_combined), 1):\n",
    "    fold_start = time.time()\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FOLD {fold_num}/5\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    X_train_svm, X_val_svm = X_tfidf_all[train_idx], X_tfidf_all[val_idx]\n",
    "    X_train_seq, X_val_seq = X_seq_all[train_idx], X_seq_all[val_idx]\n",
    "    y_train_fold, y_val_fold = y_combined.iloc[train_idx], y_combined.iloc[val_idx]\n",
    "    \n",
    "    X_train_fold = X_combined.iloc[train_idx]\n",
    "    y_train_fold = y_combined.iloc[train_idx]\n",
    "    X_val_fold = X_combined.iloc[val_idx]\n",
    "    y_val_fold = y_combined.iloc[val_idx]\n",
    "    \n",
    "    all_labels.extend(y_val_fold.values)\n",
    "    \n",
    "    # ========================================\n",
    "    # SVM (Fast - no changes needed)\n",
    "    # ========================================\n",
    "    print(\"\\n[1/3] Training SVM...\", end=\" \")\n",
    "    svm_start = time.time()\n",
    "    \n",
    "    svm = LinearSVC(random_state=42, max_iter=10000, class_weight='balanced')\n",
    "    svm.fit(X_train_svm, y_train_fold)\n",
    "    svm_preds = svm.predict(X_val_svm)\n",
    "    \n",
    "    all_predictions['svm'].extend(svm_preds)\n",
    "    svm_f1 = f1_score(y_val_fold, svm_preds)\n",
    "    model_f1_scores['svm'].append(svm_f1)\n",
    "    \n",
    "    print(f\"Done! F1: {svm_f1:.4f} ({time.time() - svm_start:.1f}s)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # BiLSTM (Optimized)\n",
    "    # ========================================\n",
    "    print(\"[2/3] Training BiLSTM...\", end=\" \")\n",
    "    bilstm_start = time.time()\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(X_train_fold)\n",
    "    X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train_fold), maxlen=100)\n",
    "    X_val_seq = pad_sequences(tokenizer.texts_to_sequences(X_val_fold), maxlen=100)\n",
    "    \n",
    "    # Change 2: Smaller model = faster training\n",
    "    bilstm = Sequential([\n",
    "        Embedding(10000, 64),  # Reduced from 128 to 64\n",
    "        Bidirectional(LSTM(32, return_sequences=True, dropout=0.3)),  # Reduced from 64 to 32\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(16, dropout=0.3)),  # Reduced from 32 to 16\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    bilstm.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Change 3: Early stopping with patience=1 (stops at first no-improvement)\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=1,  # Stop after 1 epoch with no improvement\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Change 4: Fewer max epochs (5 → 10, but early stopping will likely stop at 2-3)\n",
    "    bilstm.fit(\n",
    "        X_train_seq, y_train_fold,\n",
    "        validation_data=(X_val_seq, y_val_fold),\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    bilstm_preds = (bilstm.predict(X_val_seq, verbose=0) > 0.5).astype(int).flatten()\n",
    "    \n",
    "    all_predictions['bilstm'].extend(bilstm_preds)\n",
    "    bilstm_f1 = f1_score(y_val_fold, bilstm_preds)\n",
    "    model_f1_scores['bilstm'].append(bilstm_f1)\n",
    "    \n",
    "    print(f\"Done! F1: {bilstm_f1:.4f} ({time.time() - bilstm_start:.1f}s)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # CNN (Optimized - CNNs are naturally faster)\n",
    "    # ========================================\n",
    "    print(\"[3/3] Training CNN...\", end=\" \")\n",
    "    cnn_start = time.time()\n",
    "    \n",
    "    cnn = Sequential([\n",
    "        Embedding(10000, 64),  # Reduced from 128\n",
    "        Conv1D(64, 5, activation='relu'),  # Reduced filters from 128 to 64\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(32, activation='relu'),  # Reduced from 64 to 32\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    cnn.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # CNNs converge faster, so even fewer epochs\n",
    "    early_stop_cnn = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=1,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    cnn.fit(\n",
    "        X_train_seq, y_train_fold,\n",
    "        validation_data=(X_val_seq, y_val_fold),\n",
    "        epochs=8,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stop_cnn],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    cnn_preds = (cnn.predict(X_val_seq, verbose=0) > 0.5).astype(int).flatten()\n",
    "    \n",
    "    all_predictions['cnn'].extend(cnn_preds)\n",
    "    cnn_f1 = f1_score(y_val_fold, cnn_preds)\n",
    "    model_f1_scores['cnn'].append(cnn_f1)\n",
    "    \n",
    "    print(f\"Done! F1: {cnn_f1:.4f} ({time.time() - cnn_start:.1f}s)\")\n",
    "    \n",
    "    # Fold summary\n",
    "    fold_time = time.time() - fold_start\n",
    "    fold_times.append(fold_time)\n",
    "    \n",
    "    print(f\"\\nFold {fold_num} completed in {fold_time:.1f}s\")\n",
    "    print(f\"  SVM: {svm_f1:.4f} | BiLSTM: {bilstm_f1:.4f} | CNN: {cnn_f1:.4f}\")\n",
    "    \n",
    "    # Estimate remaining time\n",
    "    if fold_num < 5:\n",
    "        avg_time_per_fold = np.mean(fold_times)\n",
    "        remaining_folds = 5 - fold_num\n",
    "        estimated_remaining = avg_time_per_fold * remaining_folds\n",
    "        print(f\"\\n⏱️  Estimated time remaining: {estimated_remaining/60:.1f} minutes\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "avg_f1_scores = {model: np.mean(scores) for model, scores in model_f1_scores.items()}\n",
    "\n",
    "print(\"\\nAverage F1 scores across all folds:\")\n",
    "for model, f1 in sorted(avg_f1_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    std = np.std(model_f1_scores[model])\n",
    "    print(f\"  {model.upper():10s}: {f1:.4f} (±{std:.4f})\")\n",
    "\n",
    "# Calculate weights\n",
    "total_f1 = sum(avg_f1_scores.values())\n",
    "weights = {model: f1 / total_f1 for model, f1 in avg_f1_scores.items()}\n",
    "\n",
    "print(\"\\nCalculated weights (based on F1 performance):\")\n",
    "for model, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {model.upper():10s}: {weight:.4f} ({weight*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal training time: {sum(fold_times)/60:.1f} minutes\")\n",
    "print(f\"Average time per fold: {np.mean(fold_times):.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2964db94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m ensemble_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_labels))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_labels)):\n\u001b[0;32m----> 9\u001b[0m     ensemble_probs[i] \u001b[38;5;241m=\u001b[39m (\u001b[43mweights\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m all_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     10\u001b[0m                          weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilstm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m all_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilstm\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     11\u001b[0m                          weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m all_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert probabilities to binary predictions\u001b[39;00m\n\u001b[1;32m     14\u001b[0m ensemble_preds \u001b[38;5;241m=\u001b[39m (ensemble_probs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the Ensemble Model using K-Fold Cross Validation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Compute ensemble predictions as weighted average of individual model predictions\n",
    "ensemble_probs = np.zeros(len(all_labels))\n",
    "for i in range(len(all_labels)):\n",
    "    ensemble_probs[i] = (weights['svm'] * all_predictions['svm'][i] + \n",
    "                         weights['bilstm'] * all_predictions['bilstm'][i] + \n",
    "                         weights['cnn'] * all_predictions['cnn'][i])\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "ensemble_preds = (ensemble_probs > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "ensemble_accuracy = accuracy_score(all_labels, ensemble_preds)\n",
    "ensemble_f1 = f1_score(all_labels, ensemble_preds)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE MODEL EVALUATION (K-Fold Cross Validation)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Ensemble Validation Accuracy: {ensemble_accuracy:.4f}\")\n",
    "print(f\"Ensemble Validation F1 Score: {ensemble_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, ensemble_preds, target_names=['Not Sarcastic', 'Sarcastic']))\n",
    "\n",
    "# Optional: Compare with individual models\n",
    "print(\"\\nComparison with Individual Models:\")\n",
    "print(f\"{'Model':<10} {'Accuracy':<10} {'F1 Score':<10}\")\n",
    "print(\"-\" * 30)\n",
    "for model in ['svm', 'bilstm', 'cnn']:\n",
    "    model_preds = np.array(all_predictions[model])\n",
    "    acc = accuracy_score(all_labels, model_preds)\n",
    "    f1 = f1_score(all_labels, model_preds)\n",
    "    print(f\"{model.upper():<10} {acc:<10.4f} {f1:<10.4f}\")\n",
    "print(f\"{'ENSEMBLE':<10} {ensemble_accuracy:<10.4f} {ensemble_f1:<10.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
