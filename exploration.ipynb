{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f7141c",
   "metadata": {},
   "source": [
    "# Exploration of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a58bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (21464, 2)\n",
      "Valid Shape: (716, 2)\n",
      "Test Shape: (966, 2)\n",
      "\n",
      "                                                text  label\n",
      "0  states slow to shut down weak teacher educatio...      0\n",
      "1    drone places fresh kill on steps of white house      1\n",
      "2  report: majority of instances of people gettin...      1\n",
      "3  sole remaining lung filled with rich, satisfyi...      1\n",
      "4                       the gop's stockholm syndrome      0\n",
      "\n",
      "label\n",
      "0    11248\n",
      "1    10216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('./datasets/train.csv')\n",
    "valid_df = pd.read_csv('./datasets/valid.csv')\n",
    "test_df = pd.read_csv('./datasets/test.csv')\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "print(f\"Valid Shape: {valid_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Preview the training data\n",
    "print(train_df.head())\n",
    "print()\n",
    "\n",
    "# Check for class balance in the training set\n",
    "print(train_df['label'].value_counts())\n",
    "print()\n",
    "\n",
    "# Check for any missing values\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e3ea2",
   "metadata": {},
   "source": [
    "Baseline Model with Bag of Words and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d2f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy: 0.7570\n",
      "Baseline Validation F1 Score: 0.7464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1. Preprocessing: Convert text to numerical vectors (Bag of Words)\n",
    "# We limit to the top 5000 most frequent words to keep it simple\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit on training data, then transform valid and test data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_valid = vectorizer.transform(valid_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# 2. Model: Train a simple Logistic Regression model\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluation: specific metrics on validation set\n",
    "valid_preds = baseline_model.predict(X_valid)\n",
    "\n",
    "print(f\"Baseline Validation Accuracy: {accuracy_score(y_valid, valid_preds):.4f}\")\n",
    "print(f\"Baseline Validation F1 Score: {f1_score(y_valid, valid_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973c201",
   "metadata": {},
   "source": [
    "Feature Engineering with TF-IDF and N-grams\n",
    "\n",
    "Idea: weight words to give less importance to common words and more importance to unique words\n",
    "\n",
    "-> This might signal sarcasm?\n",
    "\n",
    "N-gram Idea: model seeing pairs of words together might be important for sarcasm b/c gives more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0aa649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Bigram Accuracy: 0.7737\n",
      "TF-IDF + Bigram F1 Score: 0.7632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Preprocessing: Use TF-IDF and include Bigrams (1-word and 2-word combinations)\n",
    "# We increase max_features slightly to accommodate new bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(valid_df['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# 2. Model: Retrain Logistic Regression on these new features\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "tfidf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Evaluation\n",
    "valid_preds_tfidf = tfidf_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"TF-IDF + Bigram Accuracy: {accuracy_score(y_valid, valid_preds_tfidf):.4f}\")\n",
    "print(f\"TF-IDF + Bigram F1 Score: {f1_score(y_valid, valid_preds_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb4b9e",
   "metadata": {},
   "source": [
    "Support Vector Machine\n",
    "Idea: Good at classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f26cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.7835\n",
      "SVM Validation F1 Score: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1. Model: Support Vector Machine (Linear Kernel)\n",
    "# LinearSVC is faster and often better for text than standard SVC\n",
    "svm_model = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Evaluation\n",
    "valid_preds_svm = svm_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"SVM Validation Accuracy: {accuracy_score(y_valid, valid_preds_svm):.4f}\")\n",
    "print(f\"SVM Validation F1 Score: {f1_score(y_valid, valid_preds_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce1025",
   "metadata": {},
   "source": [
    "LSTM: process text as a sequence rather than Bag of Words\n",
    "\n",
    "Idea: capture more structure in the sarcasm string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06d5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.7863 - loss: 0.4407 - val_accuracy: 0.8380 - val_loss: 0.3551\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.8010 - loss: 0.4872 - val_accuracy: 0.8087 - val_loss: 0.4256\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9011 - loss: 0.2504 - val_accuracy: 0.8310 - val_loss: 0.4001\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9374 - loss: 0.1724 - val_accuracy: 0.8310 - val_loss: 0.4220\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9545 - loss: 0.1311 - val_accuracy: 0.8268 - val_loss: 0.4614\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "LSTM Validation Accuracy: 0.8268\n",
      "LSTM Validation F1 Score: 0.8203\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 1. Preprocessing: Convert text to sequences of integers\n",
    "# We limit the vocab to 10,000 words and sequence length to 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=100)\n",
    "X_valid_seq = pad_sequences(tokenizer.texts_to_sequences(valid_df['text']), maxlen=100)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=100)\n",
    "\n",
    "# 2. Model: Define a simple LSTM network \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 4. Evaluation\n",
    "lstm_probs = model.predict(X_valid_seq)\n",
    "lstm_preds = (lstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Validation Accuracy: {accuracy_score(y_valid, lstm_preds):.4f}\")\n",
    "print(f\"LSTM Validation F1 Score: {f1_score(y_valid, lstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79784e34",
   "metadata": {},
   "source": [
    "BiLSTM to read strings bidirectionally\n",
    "\n",
    "Idea: typically better to gain more information.\n",
    "\n",
    "Optimization: Dropout so that the network is not overly reliant on specific features (prevents overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28203e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 30ms/step - accuracy: 0.7608 - loss: 0.4782 - val_accuracy: 0.8547 - val_loss: 0.3570\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.8392 - loss: 0.3747 - val_accuracy: 0.8506 - val_loss: 0.3647\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.9218 - loss: 0.2118 - val_accuracy: 0.8547 - val_loss: 0.3804\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.9451 - loss: 0.1594 - val_accuracy: 0.8408 - val_loss: 0.4291\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9585 - loss: 0.1234 - val_accuracy: 0.8366 - val_loss: 0.4916\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Bi-LSTM Validation Accuracy: 0.8366\n",
      "Bi-LSTM Validation F1 Score: 0.8302\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "\n",
    "# 1. Model: Define a Bidirectional LSTM with Dropout\n",
    "# We wrap the LSTM layer in 'Bidirectional'\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same sequence data (X_train_seq) prepared in the previous step\n",
    "bilstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 3. Evaluation\n",
    "bilstm_probs = bilstm_model.predict(X_valid_seq)\n",
    "bilstm_preds = (bilstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"Bi-LSTM Validation Accuracy: {accuracy_score(y_valid, bilstm_preds):.4f}\")\n",
    "print(f\"Bi-LSTM Validation F1 Score: {f1_score(y_valid, bilstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ca957",
   "metadata": {},
   "source": [
    "Why did the accuracy go down bro :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d922d2",
   "metadata": {},
   "source": [
    "Build Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3696d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy: 0.8394\n",
      "Ensemble Validation F1 Score: 0.8336\n"
     ]
    }
   ],
   "source": [
    "# 1. Gather predictions from our top 3 models\n",
    "# Note: We flatten the neural network arrays to make them match the SVM's shape\n",
    "pred_1 = valid_preds_svm\n",
    "pred_2 = lstm_preds.flatten()\n",
    "pred_3 = bilstm_preds.flatten()\n",
    "\n",
    "# 2. Voting Logic: Sum the predictions\n",
    "# If sum is 2 or 3, it means the majority voted 1 (Sarcasm)\n",
    "total_votes = pred_1 + pred_2 + pred_3\n",
    "ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Evaluation\n",
    "print(f\"Ensemble Validation Accuracy: {accuracy_score(y_valid, ensemble_preds):.4f}\")\n",
    "print(f\"Ensemble Validation F1 Score: {f1_score(y_valid, ensemble_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9f4fb",
   "metadata": {},
   "source": [
    "Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f454a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/31\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Final Evaluation on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8611    0.8840    0.8724       526\n",
      "           1     0.8568    0.8295    0.8430       440\n",
      "\n",
      "    accuracy                         0.8592       966\n",
      "   macro avg     0.8590    0.8568    0.8577       966\n",
      "weighted avg     0.8592    0.8592    0.8590       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[465  61]\n",
      " [ 75 365]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Get predictions for the Test Set from all 3 models\n",
    "# SVM (uses TF-IDF features)\n",
    "pred_test_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# LSTM (uses Sequence features)\n",
    "pred_test_lstm = (model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Bi-LSTM (uses Sequence features)\n",
    "pred_test_bilstm = (bilstm_model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# 2. Ensemble Voting (Majority Vote)\n",
    "# Sum the predictions (0 or 1). If sum is 2 or 3, majority is 1.\n",
    "test_votes = pred_test_svm + pred_test_lstm + pred_test_bilstm\n",
    "pred_test_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Report detailed metrics\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "print(classification_report(test_df['label'], pred_test_ensemble, digits=4))\n",
    "\n",
    "# 4. Confusion Matrix (Row: True, Col: Predicted)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], pred_test_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e01676",
   "metadata": {},
   "source": [
    "Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6f4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4010c3a5",
   "metadata": {},
   "source": [
    "# Trying Improvements to the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bcfcc",
   "metadata": {},
   "source": [
    "Text Preprocessing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46cb56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 1. Feature Extraction (Run this on RAW text to capture capitalization/punctuation)\n",
    "def extract_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    # Punctuation counts (sarcasm indicators)\n",
    "    features['exclamation_count'] = df['text'].str.count('!')\n",
    "    features['question_count'] = df['text'].str.count('\\?')\n",
    "    features['ellipsis_count'] = df['text'].str.count(r'\\.\\.\\.')\n",
    "    \n",
    "    # Capitalization (sarcasm often uses ALL CAPS or Mixed Caps)\n",
    "    features['capital_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n",
    "    features['has_all_caps_word'] = df['text'].str.contains(r'\\b[A-Z]{2,}\\b').astype(int)\n",
    "    \n",
    "    # Length metrics\n",
    "    features['text_length'] = df['text'].str.len()\n",
    "    features['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features now\n",
    "train_features = extract_features(train_df)\n",
    "valid_features = extract_features(valid_df)\n",
    "test_features = extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ab55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Clean whitespace\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing (This modifies the data for subsequent steps)\n",
    "train_df['clean_text'] = train_df['text'].apply(preprocess_text)\n",
    "valid_df['clean_text'] = valid_df['text'].apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e888d0a",
   "metadata": {},
   "source": [
    "TF-IDF on pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cdd68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved SVM Accuracy: 0.7947\n",
      "Improved SVM F1 Score: 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. New TF-IDF on Cleaned Text\n",
    "# Re-running vectorizer on the cleaner text\n",
    "tfidf_clean = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf_clean = tfidf_clean.fit_transform(train_df['clean_text'])\n",
    "X_valid_tfidf_clean = tfidf_clean.transform(valid_df['clean_text'])\n",
    "X_test_tfidf_clean = tfidf_clean.transform(test_df['clean_text'])\n",
    "\n",
    "# 4. Combine Features (TF-IDF + Manual Features)\n",
    "X_train_combined = hstack([X_train_tfidf_clean, train_features])\n",
    "X_valid_combined = hstack([X_valid_tfidf_clean, valid_features])\n",
    "X_test_combined = hstack([X_test_tfidf_clean, test_features])\n",
    "\n",
    "# 5. Train Improved SVM\n",
    "svm_model_improved = LinearSVC(random_state=42, max_iter=10000, C=1.0)\n",
    "svm_model_improved.fit(X_train_combined, y_train)\n",
    "\n",
    "# Evaluation\n",
    "valid_preds_svm_improved = svm_model_improved.predict(X_valid_combined)\n",
    "print(f\"Improved SVM Accuracy: {accuracy_score(y_valid, valid_preds_svm_improved):.4f}\")\n",
    "print(f\"Improved SVM F1 Score: {f1_score(y_valid, valid_preds_svm_improved):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2942b",
   "metadata": {},
   "source": [
    "Stacked BiDirectional LSTM + Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dfd9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 146ms/step - accuracy: 0.7417 - loss: 0.5362 - val_accuracy: 0.7332 - val_loss: 0.5126\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 162ms/step - accuracy: 0.8659 - loss: 0.3467 - val_accuracy: 0.8520 - val_loss: 0.3786\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 147ms/step - accuracy: 0.9244 - loss: 0.2123 - val_accuracy: 0.8520 - val_loss: 0.3973\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 152ms/step - accuracy: 0.9559 - loss: 0.1349 - val_accuracy: 0.8408 - val_loss: 0.5463\n",
      "Epoch 5/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 151ms/step - accuracy: 0.9765 - loss: 0.0813 - val_accuracy: 0.8254 - val_loss: 0.6230\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n",
      "Improved Bi-LSTM Accuracy: 0.8520\n",
      "Improved Bi-LSTM F1 Score: 0.8503\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Update Sequences: Use the 'clean_text' from the previous step\n",
    "# We re-fit the tokenizer on the cleaner text\n",
    "tokenizer_clean = Tokenizer(num_words=10000)\n",
    "tokenizer_clean.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "X_train_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(train_df['clean_text']), maxlen=100)\n",
    "X_valid_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(valid_df['clean_text']), maxlen=100)\n",
    "X_test_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(test_df['clean_text']), maxlen=100)\n",
    "\n",
    "# 2. Improved Model: Stacked Bi-LSTM\n",
    "# We stack two Bidirectional LSTM layers to learn more complex patterns\n",
    "bilstm_improved = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)), # Return sequences is required to stack another LSTM\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_improved.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training with Early Stopping\n",
    "# Stop training if validation loss doesn't improve for 3 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "bilstm_improved.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 4. Evaluation\n",
    "probs_bilstm_improved = bilstm_improved.predict(X_valid_seq_clean)\n",
    "preds_bilstm_improved = (probs_bilstm_improved > 0.5).astype(int)\n",
    "\n",
    "print(f\"Improved Bi-LSTM Accuracy: {accuracy_score(y_valid, preds_bilstm_improved):.4f}\")\n",
    "print(f\"Improved Bi-LSTM F1 Score: {f1_score(y_valid, preds_bilstm_improved):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665063f1",
   "metadata": {},
   "source": [
    "CNN for text classification to spot N-gram patterns\n",
    "\n",
    "Idea: detect phrases for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20be88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7926 - loss: 0.4359 - val_accuracy: 0.8561 - val_loss: 0.3426\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9230 - loss: 0.2105 - val_accuracy: 0.8464 - val_loss: 0.3764\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9729 - loss: 0.0873 - val_accuracy: 0.8310 - val_loss: 0.4747\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9920 - loss: 0.0326 - val_accuracy: 0.8324 - val_loss: 0.7039\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "CNN Validation Accuracy: 0.8561\n",
      "CNN Validation F1 Score: 0.8539\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# 1. Model: 1D Convolutional Neural Network\n",
    "# - Conv1D with kernel_size=5 looks at 5-word windows to find sarcastic phrases\n",
    "# - GlobalMaxPooling1D keeps only the strongest signal found in the text\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same 'clean' sequences and early stopping as before\n",
    "cnn_model.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 3. Evaluation\n",
    "probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "preds_cnn = (probs_cnn > 0.5).astype(int)\n",
    "\n",
    "print(f\"CNN Validation Accuracy: {accuracy_score(y_valid, preds_cnn):.4f}\")\n",
    "print(f\"CNN Validation F1 Score: {f1_score(y_valid, preds_cnn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42589f0d",
   "metadata": {},
   "source": [
    "Improved Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be768fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Final Ensemble Validation Accuracy: 0.8547\n",
      "Final Ensemble Validation F1 Score: 0.8501\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "FINAL TEST SET RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8729    0.8745    0.8737       526\n",
      "           1     0.8497    0.8477    0.8487       440\n",
      "\n",
      "    accuracy                         0.8623       966\n",
      "   macro avg     0.8613    0.8611    0.8612       966\n",
      "weighted avg     0.8623    0.8623    0.8623       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[460  66]\n",
      " [ 67 373]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Validation Set Ensemble ---\n",
    "\n",
    "# Get predictions from the 3 improved models on Validation Data\n",
    "# SVM (uses Combined Sparse Features)\n",
    "val_pred_svm = svm_model_improved.predict(X_valid_combined)\n",
    "\n",
    "# Bi-LSTM (uses Clean Sequences)\n",
    "val_probs_bilstm = bilstm_improved.predict(X_valid_seq_clean)\n",
    "val_pred_bilstm = (val_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# CNN (uses Clean Sequences)\n",
    "val_probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "val_pred_cnn = (val_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# Majority Vote Ensemble\n",
    "val_votes = val_pred_svm + val_pred_bilstm + val_pred_cnn\n",
    "val_pred_ensemble = (val_votes >= 2).astype(int)\n",
    "\n",
    "print(f\"Final Ensemble Validation Accuracy: {accuracy_score(y_valid, val_pred_ensemble):.4f}\")\n",
    "print(f\"Final Ensemble Validation F1 Score: {f1_score(y_valid, val_pred_ensemble):.4f}\")\n",
    "\n",
    "# --- 2. Test Set Evaluation (The Deliverable) ---\n",
    "\n",
    "# Get predictions on Test Data\n",
    "test_pred_svm = svm_model_improved.predict(X_test_combined)\n",
    "\n",
    "test_probs_bilstm = bilstm_improved.predict(X_test_seq_clean)\n",
    "test_pred_bilstm = (test_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# Majority Vote Ensemble\n",
    "test_votes = test_pred_svm + test_pred_bilstm + test_pred_cnn\n",
    "test_pred_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# Report\n",
    "print(\"FINAL TEST SET RESULTS:\")\n",
    "print(classification_report(test_df['label'], test_pred_ensemble, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], test_pred_ensemble))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
