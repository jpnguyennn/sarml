{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f7141c",
   "metadata": {},
   "source": [
    "# Exploration of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a58bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (21464, 2)\n",
      "Valid Shape: (716, 2)\n",
      "Test Shape: (966, 2)\n",
      "\n",
      "                                                text  label\n",
      "0  states slow to shut down weak teacher educatio...      0\n",
      "1    drone places fresh kill on steps of white house      1\n",
      "2  report: majority of instances of people gettin...      1\n",
      "3  sole remaining lung filled with rich, satisfyi...      1\n",
      "4                       the gop's stockholm syndrome      0\n",
      "\n",
      "label\n",
      "0    11248\n",
      "1    10216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('./datasets/train.csv')\n",
    "valid_df = pd.read_csv('./datasets/valid.csv')\n",
    "test_df = pd.read_csv('./datasets/test.csv')\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "print(f\"Valid Shape: {valid_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Preview the training data\n",
    "print(train_df.head())\n",
    "print()\n",
    "\n",
    "# Check for class balance in the training set\n",
    "print(train_df['label'].value_counts())\n",
    "print()\n",
    "\n",
    "# Check for any missing values\n",
    "print(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e3ea2",
   "metadata": {},
   "source": [
    "Baseline Model with Bag of Words and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d2f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy: 0.7570\n",
      "Baseline Validation F1 Score: 0.7464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1. Preprocessing: Convert text to numerical vectors (Bag of Words)\n",
    "# We limit to the top 5000 most frequent words to keep it simple\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit on training data, then transform valid and test data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_valid = vectorizer.transform(valid_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# 2. Model: Train a simple Logistic Regression model\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluation: specific metrics on validation set\n",
    "valid_preds = baseline_model.predict(X_valid)\n",
    "\n",
    "print(f\"Baseline Validation Accuracy: {accuracy_score(y_valid, valid_preds):.4f}\")\n",
    "print(f\"Baseline Validation F1 Score: {f1_score(y_valid, valid_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973c201",
   "metadata": {},
   "source": [
    "Feature Engineering with TF-IDF and N-grams\n",
    "\n",
    "Idea: weight words to give less importance to common words and more importance to unique words\n",
    "\n",
    "-> This might signal sarcasm?\n",
    "\n",
    "N-gram Idea: model seeing pairs of words together might be important for sarcasm b/c gives more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0aa649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Bigram Accuracy: 0.7737\n",
      "TF-IDF + Bigram F1 Score: 0.7632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Preprocessing: Use TF-IDF and include Bigrams (1-word and 2-word combinations)\n",
    "# We increase max_features slightly to accommodate new bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(valid_df['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# 2. Model: Retrain Logistic Regression on these new features\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "tfidf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Evaluation\n",
    "valid_preds_tfidf = tfidf_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"TF-IDF + Bigram Accuracy: {accuracy_score(y_valid, valid_preds_tfidf):.4f}\")\n",
    "print(f\"TF-IDF + Bigram F1 Score: {f1_score(y_valid, valid_preds_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb4b9e",
   "metadata": {},
   "source": [
    "Support Vector Machine\n",
    "Idea: Good at classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f26cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.7835\n",
      "SVM Validation F1 Score: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1. Model: Support Vector Machine (Linear Kernel)\n",
    "# LinearSVC is faster and often better for text than standard SVC\n",
    "svm_model = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Evaluation\n",
    "valid_preds_svm = svm_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"SVM Validation Accuracy: {accuracy_score(y_valid, valid_preds_svm):.4f}\")\n",
    "print(f\"SVM Validation F1 Score: {f1_score(y_valid, valid_preds_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce1025",
   "metadata": {},
   "source": [
    "LSTM: process text as a sequence rather than Bag of Words\n",
    "\n",
    "Idea: capture more structure in the sarcasm string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06d5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.7770 - loss: 0.4632 - val_accuracy: 0.8506 - val_loss: 0.3589\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9004 - loss: 0.2505 - val_accuracy: 0.8408 - val_loss: 0.3695\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9353 - loss: 0.1762 - val_accuracy: 0.8268 - val_loss: 0.4283\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9530 - loss: 0.1342 - val_accuracy: 0.8324 - val_loss: 0.4663\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9636 - loss: 0.1059 - val_accuracy: 0.8310 - val_loss: 0.5059\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "LSTM Validation Accuracy: 0.8310\n",
      "LSTM Validation F1 Score: 0.8322\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 1. Preprocessing: Convert text to sequences of integers\n",
    "# We limit the vocab to 10,000 words and sequence length to 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=100)\n",
    "X_valid_seq = pad_sequences(tokenizer.texts_to_sequences(valid_df['text']), maxlen=100)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=100)\n",
    "\n",
    "# 2. Model: Define a simple LSTM network \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 4. Evaluation\n",
    "lstm_probs = model.predict(X_valid_seq)\n",
    "lstm_preds = (lstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Validation Accuracy: {accuracy_score(y_valid, lstm_preds):.4f}\")\n",
    "print(f\"LSTM Validation F1 Score: {f1_score(y_valid, lstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79784e34",
   "metadata": {},
   "source": [
    "BiLSTM to read strings bidirectionally\n",
    "\n",
    "Idea: typically better to gain more information.\n",
    "\n",
    "Optimization: Dropout so that the network is not overly reliant on specific features (prevents overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28203e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 28ms/step - accuracy: 0.7620 - loss: 0.4814 - val_accuracy: 0.8492 - val_loss: 0.3483\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9009 - loss: 0.2549 - val_accuracy: 0.8450 - val_loss: 0.3517\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9357 - loss: 0.1814 - val_accuracy: 0.8478 - val_loss: 0.3977\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9519 - loss: 0.1392 - val_accuracy: 0.8310 - val_loss: 0.4543\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9646 - loss: 0.1045 - val_accuracy: 0.8352 - val_loss: 0.5630\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Bi-LSTM Validation Accuracy: 0.8352\n",
      "Bi-LSTM Validation F1 Score: 0.8309\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "\n",
    "# 1. Model: Define a Bidirectional LSTM with Dropout\n",
    "# We wrap the LSTM layer in 'Bidirectional'\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same sequence data (X_train_seq) prepared in the previous step\n",
    "bilstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 3. Evaluation\n",
    "bilstm_probs = bilstm_model.predict(X_valid_seq)\n",
    "bilstm_preds = (bilstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"Bi-LSTM Validation Accuracy: {accuracy_score(y_valid, bilstm_preds):.4f}\")\n",
    "print(f\"Bi-LSTM Validation F1 Score: {f1_score(y_valid, bilstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ca957",
   "metadata": {},
   "source": [
    "Why did the accuracy go down bro :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d922d2",
   "metadata": {},
   "source": [
    "Build Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3696d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy: 0.8352\n",
      "Ensemble Validation F1 Score: 0.8324\n"
     ]
    }
   ],
   "source": [
    "# 1. Gather predictions from our top 3 models\n",
    "# Note: We flatten the neural network arrays to make them match the SVM's shape\n",
    "pred_1 = valid_preds_svm\n",
    "pred_2 = lstm_preds.flatten()\n",
    "pred_3 = bilstm_preds.flatten()\n",
    "\n",
    "# 2. Voting Logic: Sum the predictions\n",
    "# If sum is 2 or 3, it means the majority voted 1 (Sarcasm)\n",
    "total_votes = pred_1 + pred_2 + pred_3\n",
    "ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Evaluation\n",
    "print(f\"Ensemble Validation Accuracy: {accuracy_score(y_valid, ensemble_preds):.4f}\")\n",
    "print(f\"Ensemble Validation F1 Score: {f1_score(y_valid, ensemble_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9f4fb",
   "metadata": {},
   "source": [
    "Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f454a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Final Evaluation on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8779    0.8612    0.8695       526\n",
      "           1     0.8378    0.8568    0.8472       440\n",
      "\n",
      "    accuracy                         0.8592       966\n",
      "   macro avg     0.8578    0.8590    0.8583       966\n",
      "weighted avg     0.8596    0.8592    0.8593       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[453  73]\n",
      " [ 63 377]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Get predictions for the Test Set from all 3 models\n",
    "# SVM (uses TF-IDF features)\n",
    "pred_test_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# LSTM (uses Sequence features)\n",
    "pred_test_lstm = (model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Bi-LSTM (uses Sequence features)\n",
    "pred_test_bilstm = (bilstm_model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# 2. Ensemble Voting (Majority Vote)\n",
    "# Sum the predictions (0 or 1). If sum is 2 or 3, majority is 1.\n",
    "test_votes = pred_test_svm + pred_test_lstm + pred_test_bilstm\n",
    "pred_test_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Report detailed metrics\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "print(classification_report(test_df['label'], pred_test_ensemble, digits=4))\n",
    "\n",
    "# 4. Confusion Matrix (Row: True, Col: Predicted)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], pred_test_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e01676",
   "metadata": {},
   "source": [
    "Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6f4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4010c3a5",
   "metadata": {},
   "source": [
    "# Trying Improvements to the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bcfcc",
   "metadata": {},
   "source": [
    "Text Preprocessing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46cb56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 1. Feature Extraction (Run this on RAW text to capture capitalization/punctuation)\n",
    "def extract_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    # Punctuation counts (sarcasm indicators)\n",
    "    features['exclamation_count'] = df['text'].str.count('!')\n",
    "    features['question_count'] = df['text'].str.count('\\?')\n",
    "    features['ellipsis_count'] = df['text'].str.count(r'\\.\\.\\.')\n",
    "    \n",
    "    # Capitalization (sarcasm often uses ALL CAPS or Mixed Caps)\n",
    "    features['capital_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n",
    "    features['has_all_caps_word'] = df['text'].str.contains(r'\\b[A-Z]{2,}\\b').astype(int)\n",
    "    \n",
    "    # Length metrics\n",
    "    features['text_length'] = df['text'].str.len()\n",
    "    features['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features now\n",
    "train_features = extract_features(train_df)\n",
    "valid_features = extract_features(valid_df)\n",
    "test_features = extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ab55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Clean whitespace\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing (This modifies the data for subsequent steps)\n",
    "train_df['clean_text'] = train_df['text'].apply(preprocess_text)\n",
    "valid_df['clean_text'] = valid_df['text'].apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e888d0a",
   "metadata": {},
   "source": [
    "TF-IDF on pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdd68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved SVM Accuracy: 0.7751\n",
      "Improved SVM F1 Score: 0.7504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. New TF-IDF on Cleaned Text\n",
    "# Re-running vectorizer on the cleaner text\n",
    "tfidf_clean = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf_clean = tfidf_clean.fit_transform(train_df['clean_text'])\n",
    "X_valid_tfidf_clean = tfidf_clean.transform(valid_df['clean_text'])\n",
    "X_test_tfidf_clean = tfidf_clean.transform(test_df['clean_text'])\n",
    "\n",
    "# 4. Combine Features (TF-IDF + Manual Features)\n",
    "X_train_combined = hstack([X_train_tfidf_clean, train_features])\n",
    "X_valid_combined = hstack([X_valid_tfidf_clean, valid_features])\n",
    "X_test_combined = hstack([X_test_tfidf_clean, test_features])\n",
    "\n",
    "# 5. Train Improved SVM\n",
    "svm_model_improved = LinearSVC(random_state=42, max_iter=10000, C=1.0)\n",
    "svm_model_improved.fit(X_train_combined, y_train)\n",
    "\n",
    "# Evaluation\n",
    "valid_preds_svm_improved = svm_model_improved.predict(X_valid_combined)\n",
    "print(f\"Improved SVM Accuracy: {accuracy_score(y_valid, valid_preds_svm_improved):.4f}\")\n",
    "print(f\"Improved SVM F1 Score: {f1_score(y_valid, valid_preds_svm_improved):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2942b",
   "metadata": {},
   "source": [
    "Stacked BiDirectional LSTM + Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dfd9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 124ms/step - accuracy: 0.7885 - loss: 0.4463 - val_accuracy: 0.8589 - val_loss: 0.3360\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 141ms/step - accuracy: 0.9146 - loss: 0.2319 - val_accuracy: 0.8492 - val_loss: 0.3866\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 139ms/step - accuracy: 0.9541 - loss: 0.1324 - val_accuracy: 0.8394 - val_loss: 0.4783\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 147ms/step - accuracy: 0.9751 - loss: 0.0783 - val_accuracy: 0.8142 - val_loss: 0.7130\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "Improved Bi-LSTM Accuracy: 0.8589\n",
      "Improved Bi-LSTM F1 Score: 0.8571\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Update Sequences: Use the 'clean_text' from the previous step\n",
    "# We re-fit the tokenizer on the cleaner text\n",
    "tokenizer_clean = Tokenizer(num_words=10000)\n",
    "tokenizer_clean.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "X_train_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(train_df['clean_text']), maxlen=100)\n",
    "X_valid_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(valid_df['clean_text']), maxlen=100)\n",
    "X_test_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(test_df['clean_text']), maxlen=100)\n",
    "\n",
    "# 2. Improved Model: Stacked Bi-LSTM\n",
    "# We stack two Bidirectional LSTM layers to learn more complex patterns\n",
    "bilstm_improved = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)), # Return sequences is required to stack another LSTM\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_improved.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training with Early Stopping\n",
    "# Stop training if validation loss doesn't improve for 3 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "bilstm_improved.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 4. Evaluation\n",
    "probs_bilstm_improved = bilstm_improved.predict(X_valid_seq_clean)\n",
    "preds_bilstm_improved = (probs_bilstm_improved > 0.5).astype(int)\n",
    "\n",
    "print(f\"Improved Bi-LSTM Accuracy: {accuracy_score(y_valid, preds_bilstm_improved):.4f}\")\n",
    "print(f\"Improved Bi-LSTM F1 Score: {f1_score(y_valid, preds_bilstm_improved):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665063f1",
   "metadata": {},
   "source": [
    "CNN for text classification to spot N-gram patterns\n",
    "\n",
    "Idea: detect phrases for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20be88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.8003 - loss: 0.4306 - val_accuracy: 0.8547 - val_loss: 0.3322\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9223 - loss: 0.2080 - val_accuracy: 0.8506 - val_loss: 0.3709\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9741 - loss: 0.0840 - val_accuracy: 0.8338 - val_loss: 0.4929\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9929 - loss: 0.0279 - val_accuracy: 0.8324 - val_loss: 0.6885\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "CNN Validation Accuracy: 0.8547\n",
      "CNN Validation F1 Score: 0.8567\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# 1. Model: 1D Convolutional Neural Network\n",
    "# - Conv1D with kernel_size=5 looks at 5-word windows to find sarcastic phrases\n",
    "# - GlobalMaxPooling1D keeps only the strongest signal found in the text\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same 'clean' sequences and early stopping as before\n",
    "cnn_model.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 3. Evaluation\n",
    "probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "preds_cnn = (probs_cnn > 0.5).astype(int)\n",
    "\n",
    "print(f\"CNN Validation Accuracy: {accuracy_score(y_valid, preds_cnn):.4f}\")\n",
    "print(f\"CNN Validation F1 Score: {f1_score(y_valid, preds_cnn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42589f0d",
   "metadata": {},
   "source": [
    "Improved Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be768fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Final Ensemble Validation Accuracy: 0.8589\n",
      "Final Ensemble Validation F1 Score: 0.8571\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "FINAL TEST SET RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8689    0.8821    0.8755       526\n",
      "           1     0.8565    0.8409    0.8486       440\n",
      "\n",
      "    accuracy                         0.8634       966\n",
      "   macro avg     0.8627    0.8615    0.8620       966\n",
      "weighted avg     0.8633    0.8634    0.8632       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[464  62]\n",
      " [ 70 370]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Validation Set Ensemble ---\n",
    "\n",
    "# Get predictions from the 3 improved models on Validation Data\n",
    "# SVM (uses Combined Sparse Features)\n",
    "val_pred_svm = svm_model_improved.predict(X_valid_combined)\n",
    "\n",
    "# Bi-LSTM (uses Clean Sequences)\n",
    "val_probs_bilstm = bilstm_improved.predict(X_valid_seq_clean)\n",
    "val_pred_bilstm = (val_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# CNN (uses Clean Sequences)\n",
    "val_probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "val_pred_cnn = (val_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# Majority Vote Ensemble\n",
    "val_votes = val_pred_svm + val_pred_bilstm + val_pred_cnn\n",
    "val_pred_ensemble = (val_votes >= 2).astype(int)\n",
    "\n",
    "print(f\"Final Ensemble Validation Accuracy: {accuracy_score(y_valid, val_pred_ensemble):.4f}\")\n",
    "print(f\"Final Ensemble Validation F1 Score: {f1_score(y_valid, val_pred_ensemble):.4f}\")\n",
    "\n",
    "# --- 2. Test Set Evaluation (The Deliverable) ---\n",
    "\n",
    "# Get predictions on Test Data\n",
    "test_pred_svm = svm_model_improved.predict(X_test_combined)\n",
    "\n",
    "test_probs_bilstm = bilstm_improved.predict(X_test_seq_clean)\n",
    "test_pred_bilstm = (test_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# Majority Vote Ensemble\n",
    "test_votes = test_pred_svm + test_pred_bilstm + test_pred_cnn\n",
    "test_pred_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# Report\n",
    "print(\"FINAL TEST SET RESULTS:\")\n",
    "print(classification_report(test_df['label'], test_pred_ensemble, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], test_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304e6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "=== ENSEMBLE RESULTS (Original SVM + Bi-LSTM + CNN) ===\n",
      "Accuracy: 0.8654\n",
      "F1 Score: 0.8526\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8779    0.8745    0.8762       526\n",
      "           1     0.8507    0.8545    0.8526       440\n",
      "\n",
      "    accuracy                         0.8654       966\n",
      "   macro avg     0.8643    0.8645    0.8644       966\n",
      "weighted avg     0.8655    0.8654    0.8654       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[460  66]\n",
      " [ 64 376]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Get Predictions for the Test Set ---\n",
    "\n",
    "# Model A: ORIGINAL SVM (Using TF-IDF features from snippet 1)\n",
    "# Assuming 'svm_model' is your original model\n",
    "test_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Model B: BI-LSTM (Using clean sequences from snippet 2)\n",
    "test_probs_bilstm = bilstm_improved.predict(X_test_seq_clean)\n",
    "test_pred_bilstm = (test_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# Model C: CNN (Using clean sequences from snippet 2)\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# --- 2. Voting Logic (Hard Voting) ---\n",
    "\n",
    "# Sum the binary predictions (0 or 1)\n",
    "# Possible sums: 0, 1 (Majority 0) | 2, 3 (Majority 1)\n",
    "test_votes = test_pred_svm + test_pred_bilstm + test_pred_cnn\n",
    "final_ensemble_preds = (test_votes >= 2).astype(int)\n",
    "\n",
    "# --- 3. Final Evaluation ---\n",
    "\n",
    "print(\"=== ENSEMBLE RESULTS (Original SVM + Bi-LSTM + CNN) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(test_df['label'], final_ensemble_preds):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(test_df['label'], final_ensemble_preds):.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_df['label'], final_ensemble_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], final_ensemble_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
