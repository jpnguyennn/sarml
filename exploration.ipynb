{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f7141c",
   "metadata": {},
   "source": [
    "# Exploration of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a58bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (21464, 2)\n",
      "Valid Shape: (716, 2)\n",
      "Test Shape: (966, 2)\n",
      "\n",
      "                                                text  label\n",
      "0  states slow to shut down weak teacher educatio...      0\n",
      "1    drone places fresh kill on steps of white house      1\n",
      "2  report: majority of instances of people gettin...      1\n",
      "3  sole remaining lung filled with rich, satisfyi...      1\n",
      "4                       the gop's stockholm syndrome      0\n",
      "\n",
      "label\n",
      "0    11248\n",
      "1    10216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('./datasets/train.csv')\n",
    "valid_df = pd.read_csv('./datasets/valid.csv')\n",
    "test_df = pd.read_csv('./datasets/test.csv')\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "print(f\"Valid Shape: {valid_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Preview the training data\n",
    "print(train_df.head())\n",
    "print()\n",
    "\n",
    "# Check for class balance in the training set\n",
    "print(train_df['label'].value_counts())\n",
    "print()\n",
    "\n",
    "# Check for any missing values\n",
    "print(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e3ea2",
   "metadata": {},
   "source": [
    "Baseline Model with Bag of Words and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d2f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy: 0.7598\n",
      "Baseline Validation F1 Score: 0.7485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1. Preprocessing: Convert text to numerical vectors (Bag of Words)\n",
    "# We limit to the top 5000 most frequent words to keep it simple\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit on training data, then transform valid and test data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_valid = vectorizer.transform(valid_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# 2. Model: Train a simple Logistic Regression model\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluation: specific metrics on validation set\n",
    "valid_preds = baseline_model.predict(X_valid)\n",
    "\n",
    "print(f\"Baseline Validation Accuracy: {accuracy_score(y_valid, valid_preds):.4f}\")\n",
    "print(f\"Baseline Validation F1 Score: {f1_score(y_valid, valid_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973c201",
   "metadata": {},
   "source": [
    "Feature Engineering with TF-IDF and N-grams\n",
    "\n",
    "Idea: weight words to give less importance to common words and more importance to unique words\n",
    "\n",
    "-> This might signal sarcasm?\n",
    "\n",
    "N-gram Idea: model seeing pairs of words together might be important for sarcasm b/c gives more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0aa649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Bigram Accuracy: 0.7709\n",
      "TF-IDF + Bigram F1 Score: 0.7595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Preprocessing: Use TF-IDF and include Bigrams (1-word and 2-word combinations)\n",
    "# We increase max_features slightly to accommodate new bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(valid_df['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# 2. Model: Retrain Logistic Regression on these new features\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "tfidf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Evaluation\n",
    "valid_preds_tfidf = tfidf_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"TF-IDF + Bigram Accuracy: {accuracy_score(y_valid, valid_preds_tfidf):.4f}\")\n",
    "print(f\"TF-IDF + Bigram F1 Score: {f1_score(y_valid, valid_preds_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb4b9e",
   "metadata": {},
   "source": [
    "Support Vector Machine\n",
    "Idea: Good at classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f26cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.7751\n",
      "SVM Validation F1 Score: 0.7636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1. Model: Support Vector Machine (Linear Kernel)\n",
    "# LinearSVC is faster and often better for text than standard SVC\n",
    "svm_model = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Evaluation\n",
    "valid_preds_svm = svm_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"SVM Validation Accuracy: {accuracy_score(y_valid, valid_preds_svm):.4f}\")\n",
    "print(f\"SVM Validation F1 Score: {f1_score(y_valid, valid_preds_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce1025",
   "metadata": {},
   "source": [
    "LSTM: process text as a sequence rather than Bag of Words\n",
    "\n",
    "Idea: capture more structure in the sarcasm string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06d5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fieryhacker/sarml/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.7839 - loss: 0.4366 - val_accuracy: 0.8547 - val_loss: 0.3438\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.8961 - loss: 0.2816 - val_accuracy: 0.8520 - val_loss: 0.3683\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9357 - loss: 0.1728 - val_accuracy: 0.8478 - val_loss: 0.3973\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9535 - loss: 0.1293 - val_accuracy: 0.8436 - val_loss: 0.4528\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 31ms/step - accuracy: 0.9675 - loss: 0.0938 - val_accuracy: 0.8254 - val_loss: 0.5266\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "LSTM Validation Accuracy: 0.8254\n",
      "LSTM Validation F1 Score: 0.8207\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 1. Preprocessing: Convert text to sequences of integers\n",
    "# We limit the vocab to 10,000 words and sequence length to 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=100)\n",
    "X_valid_seq = pad_sequences(tokenizer.texts_to_sequences(valid_df['text']), maxlen=100)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=100)\n",
    "\n",
    "# 2. Model: Define a simple LSTM network \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 4. Evaluation\n",
    "lstm_probs = model.predict(X_valid_seq)\n",
    "lstm_preds = (lstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Validation Accuracy: {accuracy_score(y_valid, lstm_preds):.4f}\")\n",
    "print(f\"LSTM Validation F1 Score: {f1_score(y_valid, lstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79784e34",
   "metadata": {},
   "source": [
    "BiLSTM to read strings bidirectionally\n",
    "\n",
    "Idea: typically better to gain more information.\n",
    "\n",
    "Optimization: Dropout so that the network is not overly reliant on specific features (prevents overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28203e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fieryhacker/sarml/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.7676 - loss: 0.4734 - val_accuracy: 0.8422 - val_loss: 0.3555\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 36ms/step - accuracy: 0.9009 - loss: 0.2549 - val_accuracy: 0.8464 - val_loss: 0.3856\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 41ms/step - accuracy: 0.9346 - loss: 0.1796 - val_accuracy: 0.8436 - val_loss: 0.4123\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.9526 - loss: 0.1373 - val_accuracy: 0.8380 - val_loss: 0.4360\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.9674 - loss: 0.0996 - val_accuracy: 0.8310 - val_loss: 0.5597\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "Bi-LSTM Validation Accuracy: 0.8310\n",
      "Bi-LSTM Validation F1 Score: 0.8264\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "\n",
    "# 1. Model: Define a Bidirectional LSTM with Dropout\n",
    "# We wrap the LSTM layer in 'Bidirectional'\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same sequence data (X_train_seq) prepared in the previous step\n",
    "bilstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 3. Evaluation\n",
    "bilstm_probs = bilstm_model.predict(X_valid_seq)\n",
    "bilstm_preds = (bilstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"Bi-LSTM Validation Accuracy: {accuracy_score(y_valid, bilstm_preds):.4f}\")\n",
    "print(f\"Bi-LSTM Validation F1 Score: {f1_score(y_valid, bilstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ca957",
   "metadata": {},
   "source": [
    "Why did the accuracy go down bro :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d922d2",
   "metadata": {},
   "source": [
    "Build Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3696d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy: 0.8310\n",
      "Ensemble Validation F1 Score: 0.8254\n"
     ]
    }
   ],
   "source": [
    "# 1. Gather predictions from our top 3 models\n",
    "# Note: We flatten the neural network arrays to make them match the SVM's shape\n",
    "pred_1 = valid_preds_svm\n",
    "pred_2 = lstm_preds.flatten()\n",
    "pred_3 = bilstm_preds.flatten()\n",
    "\n",
    "# 2. Voting Logic: Sum the predictions\n",
    "# If sum is 2 or 3, it means the majority voted 1 (Sarcasm)\n",
    "total_votes = pred_1 + pred_2 + pred_3\n",
    "ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Evaluation\n",
    "print(f\"Ensemble Validation Accuracy: {accuracy_score(y_valid, ensemble_preds):.4f}\")\n",
    "print(f\"Ensemble Validation F1 Score: {f1_score(y_valid, ensemble_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9f4fb",
   "metadata": {},
   "source": [
    "Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f454a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Final Evaluation on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8593    0.8707    0.8650       526\n",
      "           1     0.8430    0.8295    0.8362       440\n",
      "\n",
      "    accuracy                         0.8520       966\n",
      "   macro avg     0.8511    0.8501    0.8506       966\n",
      "weighted avg     0.8518    0.8520    0.8519       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[458  68]\n",
      " [ 75 365]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Get predictions for the Test Set from all 3 models\n",
    "# SVM (uses TF-IDF features)\n",
    "pred_test_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# LSTM (uses Sequence features)\n",
    "pred_test_lstm = (model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Bi-LSTM (uses Sequence features)\n",
    "pred_test_bilstm = (bilstm_model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# 2. Ensemble Voting (Majority Vote)\n",
    "# Sum the predictions (0 or 1). If sum is 2 or 3, majority is 1.\n",
    "test_votes = pred_test_svm + pred_test_lstm + pred_test_bilstm\n",
    "pred_test_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Report detailed metrics\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "print(classification_report(test_df['label'], pred_test_ensemble, digits=4))\n",
    "\n",
    "# 4. Confusion Matrix (Row: True, Col: Predicted)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], pred_test_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e01676",
   "metadata": {},
   "source": [
    "Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6f4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4010c3a5",
   "metadata": {},
   "source": [
    "# Trying Improvements to the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bcfcc",
   "metadata": {},
   "source": [
    "Text Preprocessing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46cb56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# 1. Feature Extraction (Run this on RAW text to capture capitalization/punctuation)\n",
    "def extract_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    # Punctuation counts (sarcasm indicators)\n",
    "    features['exclamation_count'] = df['text'].str.count('!')\n",
    "    features['question_count'] = df['text'].str.count('\\?')\n",
    "    features['ellipsis_count'] = df['text'].str.count(r'\\.\\.\\.')\n",
    "    \n",
    "    # Capitalization (sarcasm often uses ALL CAPS or Mixed Caps)\n",
    "    features['capital_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n",
    "    features['has_all_caps_word'] = df['text'].str.contains(r'\\b[A-Z]{2,}\\b').astype(int)\n",
    "    \n",
    "    # Length metrics\n",
    "    features['text_length'] = df['text'].str.len()\n",
    "    features['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features now\n",
    "train_features = extract_features(train_df)\n",
    "valid_features = extract_features(valid_df)\n",
    "test_features = extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ab55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Clean whitespace\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing (This modifies the data for subsequent steps)\n",
    "train_df['clean_text'] = train_df['text'].apply(preprocess_text)\n",
    "valid_df['clean_text'] = valid_df['text'].apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e888d0a",
   "metadata": {},
   "source": [
    "TF-IDF on pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cdd68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved SVM Accuracy: 0.8017\n",
      "Improved SVM F1 Score: 0.7948\n"
     ]
    }
   ],
   "source": [
    "# 3. New TF-IDF on Cleaned Text\n",
    "# Re-running vectorizer on the cleaner text\n",
    "tfidf_clean = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf_clean = tfidf_clean.fit_transform(train_df['clean_text'])\n",
    "X_valid_tfidf_clean = tfidf_clean.transform(valid_df['clean_text'])\n",
    "X_test_tfidf_clean = tfidf_clean.transform(test_df['clean_text'])\n",
    "\n",
    "# 4. Combine Features (TF-IDF + Manual Features)\n",
    "X_train_combined = hstack([X_train_tfidf_clean, train_features])\n",
    "X_valid_combined = hstack([X_valid_tfidf_clean, valid_features])\n",
    "X_test_combined = hstack([X_test_tfidf_clean, test_features])\n",
    "\n",
    "# 5. Train Improved SVM\n",
    "svm_model_improved = LinearSVC(random_state=42, max_iter=10000, C=1.0)\n",
    "svm_model_improved.fit(X_train_combined, y_train)\n",
    "\n",
    "# Evaluation\n",
    "valid_preds_svm_improved = svm_model_improved.predict(X_valid_combined)\n",
    "print(f\"Improved SVM Accuracy: {accuracy_score(y_valid, valid_preds_svm_improved):.4f}\")\n",
    "print(f\"Improved SVM F1 Score: {f1_score(y_valid, valid_preds_svm_improved):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2942b",
   "metadata": {},
   "source": [
    "Stacked BiDirectional LSTM + Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dfd9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fieryhacker/sarml/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 103ms/step - accuracy: 0.7724 - loss: 0.4592 - val_accuracy: 0.8659 - val_loss: 0.3505\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 106ms/step - accuracy: 0.9139 - loss: 0.2324 - val_accuracy: 0.8478 - val_loss: 0.3840\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 102ms/step - accuracy: 0.9548 - loss: 0.1321 - val_accuracy: 0.8296 - val_loss: 0.4995\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 103ms/step - accuracy: 0.9745 - loss: 0.0803 - val_accuracy: 0.8254 - val_loss: 0.6515\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\n",
      "Improved Bi-LSTM Accuracy: 0.8659\n",
      "Improved Bi-LSTM F1 Score: 0.8655\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Update Sequences: Use the 'clean_text' from the previous step\n",
    "# We re-fit the tokenizer on the cleaner text\n",
    "tokenizer_clean = Tokenizer(num_words=10000)\n",
    "tokenizer_clean.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "X_train_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(train_df['clean_text']), maxlen=100)\n",
    "X_valid_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(valid_df['clean_text']), maxlen=100)\n",
    "X_test_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(test_df['clean_text']), maxlen=100)\n",
    "\n",
    "# 2. Improved Model: Stacked Bi-LSTM\n",
    "# We stack two Bidirectional LSTM layers to learn more complex patterns\n",
    "bilstm_improved = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)), # Return sequences is required to stack another LSTM\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_improved.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training with Early Stopping\n",
    "# Stop training if validation loss doesn't improve for 3 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "bilstm_improved.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 4. Evaluation\n",
    "probs_bilstm_improved = bilstm_improved.predict(X_valid_seq_clean)\n",
    "preds_bilstm_improved = (probs_bilstm_improved > 0.5).astype(int)\n",
    "\n",
    "print(f\"Improved Bi-LSTM Accuracy: {accuracy_score(y_valid, preds_bilstm_improved):.4f}\")\n",
    "print(f\"Improved Bi-LSTM F1 Score: {f1_score(y_valid, preds_bilstm_improved):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665063f1",
   "metadata": {},
   "source": [
    "CNN for text classification to spot N-gram patterns\n",
    "\n",
    "Idea: detect phrases for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20be88fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fieryhacker/sarml/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.7929 - loss: 0.4377 - val_accuracy: 0.8575 - val_loss: 0.3469\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.9185 - loss: 0.2194 - val_accuracy: 0.8394 - val_loss: 0.3881\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9699 - loss: 0.0983 - val_accuracy: 0.8254 - val_loss: 0.4905\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step - accuracy: 0.9892 - loss: 0.0389 - val_accuracy: 0.8366 - val_loss: 0.6680\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "CNN Validation Accuracy: 0.8575\n",
      "CNN Validation F1 Score: 0.8534\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# 1. Model: 1D Convolutional Neural Network\n",
    "# - Conv1D with kernel_size=5 looks at 5-word windows to find sarcastic phrases\n",
    "# - GlobalMaxPooling1D keeps only the strongest signal found in the text\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same 'clean' sequences and early stopping as before\n",
    "cnn_model.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# 3. Evaluation\n",
    "probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "preds_cnn = (probs_cnn > 0.5).astype(int)\n",
    "\n",
    "print(f\"CNN Validation Accuracy: {accuracy_score(y_valid, preds_cnn):.4f}\")\n",
    "print(f\"CNN Validation F1 Score: {f1_score(y_valid, preds_cnn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42589f0d",
   "metadata": {},
   "source": [
    "Improved Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be768fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Final Ensemble Validation Accuracy: 0.8617\n",
      "Final Ensemble Validation F1 Score: 0.8580\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "FINAL TEST SET RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8706    0.8954    0.8828       526\n",
      "           1     0.8706    0.8409    0.8555       440\n",
      "\n",
      "    accuracy                         0.8706       966\n",
      "   macro avg     0.8706    0.8682    0.8692       966\n",
      "weighted avg     0.8706    0.8706    0.8704       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[471  55]\n",
      " [ 70 370]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Validation Set Ensemble ---\n",
    "\n",
    "# Get predictions from the 3 improved models on Validation Data\n",
    "# SVM (uses Combined Sparse Features)\n",
    "val_pred_svm = svm_model_improved.predict(X_valid_combined)\n",
    "\n",
    "# Bi-LSTM (uses Clean Sequences)\n",
    "val_probs_bilstm = bilstm_improved.predict(X_valid_seq_clean)\n",
    "val_pred_bilstm = (val_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# CNN (uses Clean Sequences)\n",
    "val_probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "val_pred_cnn = (val_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# Majority Vote Ensemble\n",
    "val_votes = val_pred_svm + val_pred_bilstm + val_pred_cnn\n",
    "val_pred_ensemble = (val_votes >= 2).astype(int)\n",
    "\n",
    "print(f\"Final Ensemble Validation Accuracy: {accuracy_score(y_valid, val_pred_ensemble):.4f}\")\n",
    "print(f\"Final Ensemble Validation F1 Score: {f1_score(y_valid, val_pred_ensemble):.4f}\")\n",
    "\n",
    "# --- 2. Test Set Evaluation (The Deliverable) ---\n",
    "\n",
    "# Get predictions on Test Data\n",
    "test_pred_svm = svm_model_improved.predict(X_test_combined)\n",
    "\n",
    "test_probs_bilstm = bilstm_improved.predict(X_test_seq_clean)\n",
    "test_pred_bilstm = (test_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# Majority Vote Ensemble\n",
    "test_votes = test_pred_svm + test_pred_bilstm + test_pred_cnn\n",
    "test_pred_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# Report\n",
    "print(\"FINAL TEST SET RESULTS:\")\n",
    "print(classification_report(test_df['label'], test_pred_ensemble, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], test_pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "304e6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "=== ENSEMBLE RESULTS (Original SVM + Bi-LSTM + CNN) ===\n",
      "Accuracy: 0.8685\n",
      "F1 Score: 0.8532\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8688    0.8935    0.8810       526\n",
      "           1     0.8682    0.8386    0.8532       440\n",
      "\n",
      "    accuracy                         0.8685       966\n",
      "   macro avg     0.8685    0.8661    0.8671       966\n",
      "weighted avg     0.8685    0.8685    0.8683       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[470  56]\n",
      " [ 71 369]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- 1. Get Predictions for the Test Set ---\n",
    "\n",
    "# Model A: ORIGINAL SVM (Using TF-IDF features from snippet 1)\n",
    "# Assuming 'svm_model' is your original model\n",
    "test_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Model B: BI-LSTM (Using clean sequences from snippet 2)\n",
    "test_probs_bilstm = bilstm_improved.predict(X_test_seq_clean)\n",
    "test_pred_bilstm = (test_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# Model C: CNN (Using clean sequences from snippet 2)\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# --- 2. Voting Logic (Hard Voting) ---\n",
    "\n",
    "# Sum the binary predictions (0 or 1)\n",
    "# Possible sums: 0, 1 (Majority 0) | 2, 3 (Majority 1)\n",
    "test_votes = test_pred_svm + test_pred_bilstm + test_pred_cnn\n",
    "final_ensemble_preds = (test_votes >= 2).astype(int)\n",
    "\n",
    "# --- 3. Final Evaluation ---\n",
    "\n",
    "print(\"=== ENSEMBLE RESULTS (Original SVM + Bi-LSTM + CNN) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(test_df['label'], final_ensemble_preds):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(test_df['label'], final_ensemble_preds):.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_df['label'], final_ensemble_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], final_ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f55a8ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "=== ENSEMBLE RESULTS: SVM + LSTM + Bi-LSTM ===\n",
      "Accuracy: 0.8602\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8668    0.8783    0.8725       526\n",
      "           1     0.8522    0.8386    0.8454       440\n",
      "\n",
      "    accuracy                         0.8602       966\n",
      "   macro avg     0.8595    0.8585    0.8589       966\n",
      "weighted avg     0.8601    0.8602    0.8602       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[462  64]\n",
      " [ 71 369]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# --- 1. Generate Predictions ---\n",
    "\n",
    "# Model 1: SVM (Original)\n",
    "pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Model 2: LSTM\n",
    "# Using the standard sequence features\n",
    "pred_probs_lstm = model.predict(X_test_seq) \n",
    "pred_lstm = (pred_probs_lstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# Model 3: Bi-LSTM\n",
    "# Using the clean sequence features from your second snippet\n",
    "pred_probs_bilstm = bilstm_improved.predict(X_test_seq_clean)\n",
    "pred_bilstm = (pred_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# --- 2. Majority Vote Logic ---\n",
    "\n",
    "# We add the three arrays together. \n",
    "# A sum of 2 or 3 means at least two models predicted '1'.\n",
    "combined_votes = pred_svm + pred_lstm + pred_bilstm\n",
    "ensemble_final_preds = (combined_votes >= 2).astype(int)\n",
    "\n",
    "# --- 3. Performance Metrics ---\n",
    "\n",
    "print(\"=== ENSEMBLE RESULTS: SVM + LSTM + Bi-LSTM ===\")\n",
    "print(f\"Accuracy: {accuracy_score(test_df['label'], ensemble_final_preds):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], ensemble_final_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], ensemble_final_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cedb2657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "=== HYBRID ENSEMBLE: SVM + LSTM + CNN ===\n",
      "Ensemble Accuracy: 0.8634\n",
      "\n",
      "Detailed Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8648    0.8878    0.8762       526\n",
      "           1     0.8615    0.8341    0.8476       440\n",
      "\n",
      "    accuracy                         0.8634       966\n",
      "   macro avg     0.8632    0.8610    0.8619       966\n",
      "weighted avg     0.8633    0.8634    0.8631       966\n",
      "\n",
      "\n",
      "Confusion Matrix (Actual vs Predicted):\n",
      "[[467  59]\n",
      " [ 73 367]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# --- 1. Get Predictions ---\n",
    "\n",
    "# Model A: SVM (Original TF-IDF version)\n",
    "# Using features from snippet 1\n",
    "test_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Model B: LSTM (Sequence-based)\n",
    "# Using standard sequences from snippet 1\n",
    "test_probs_lstm = model.predict(X_test_seq)\n",
    "test_pred_lstm = (test_probs_lstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# Model C: CNN (Clean Sequence-based)\n",
    "# Using clean sequences from snippet 2\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# --- 2. Ensemble Voting Logic ---\n",
    "\n",
    "# We add the individual 0/1 predictions\n",
    "# If 2 or more models say \"1\", the result is \"1\"\n",
    "total_votes = test_pred_svm + test_pred_lstm + test_pred_cnn\n",
    "hybrid_ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# --- 3. Results & Evaluation ---\n",
    "\n",
    "print(\"=== HYBRID ENSEMBLE: SVM + LSTM + CNN ===\")\n",
    "print(f\"Ensemble Accuracy: {accuracy_score(test_df['label'], hybrid_ensemble_preds):.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Performance:\")\n",
    "print(classification_report(test_df['label'], hybrid_ensemble_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Actual vs Predicted):\")\n",
    "print(confusion_matrix(test_df['label'], hybrid_ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d324303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "=== DEEP LEARNING ENSEMBLE: LSTM + Bi-LSTM + CNN ===\n",
      "Final Accuracy: 0.8589\n",
      "\n",
      "Detailed Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8529    0.8694    0.8611       360\n",
      "           1     0.8653    0.8483    0.8567       356\n",
      "\n",
      "    accuracy                         0.8589       716\n",
      "   macro avg     0.8591    0.8589    0.8589       716\n",
      "weighted avg     0.8591    0.8589    0.8589       716\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[313  47]\n",
      " [ 54 302]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# --- 1. Get Predictions ---\n",
    "\n",
    "# Model A: LSTM\n",
    "# Using original sequence data\n",
    "probs_lstm = model.predict(X_test_seq)\n",
    "pred_lstm = (probs_lstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# Model B: Bi-LSTM\n",
    "# Using clean sequence data from your second snippet\n",
    "probs_bilstm = bilstm_improved.predict(X_test_seq_clean)\n",
    "pred_bilstm = (probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# Model C: CNN\n",
    "# Using clean sequence data from your second snippet\n",
    "probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "pred_cnn = (probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# --- 2. Ensemble Voting Logic ---\n",
    "\n",
    "# Hard Voting: Summing the binary results\n",
    "# If 2 or 3 models predict 1, the ensemble predicts 1\n",
    "total_votes = pred_lstm + pred_bilstm + pred_cnn\n",
    "deep_ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# --- 3. Evaluation ---\n",
    "\n",
    "print(\"=== DEEP LEARNING ENSEMBLE: LSTM + Bi-LSTM + CNN ===\")\n",
    "print(f\"Final Accuracy: {accuracy_score(test_df['label'], deep_ensemble_preds):.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(classification_report(test_df['label'], deep_ensemble_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], deep_ensemble_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
