{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f7141c",
   "metadata": {},
   "source": [
    "# Exploration of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5e8c3",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "770c4af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# baseline model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# SVM model\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# LSTM model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# BiLSTM model\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# CNN model\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "# statistical imports\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77bb298",
   "metadata": {},
   "source": [
    "## Loading Datasets and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a58bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (21464, 2)\n",
      "Valid Shape: (716, 2)\n",
      "Test Shape: (966, 2)\n",
      "\n",
      "                                                text  label\n",
      "0  states slow to shut down weak teacher educatio...      0\n",
      "1    drone places fresh kill on steps of white house      1\n",
      "2  report: majority of instances of people gettin...      1\n",
      "3  sole remaining lung filled with rich, satisfyi...      1\n",
      "4                       the gop's stockholm syndrome      0\n",
      "\n",
      "label\n",
      "0    11248\n",
      "1    10216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('./datasets/train.csv')\n",
    "valid_df = pd.read_csv('./datasets/valid.csv')\n",
    "test_df = pd.read_csv('./datasets/test.csv')\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "print(f\"Valid Shape: {valid_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Preview the training data\n",
    "print(train_df.head())\n",
    "print()\n",
    "\n",
    "# Check for class balance in the training set\n",
    "print(train_df['label'].value_counts())\n",
    "print()\n",
    "\n",
    "# Check for any missing values\n",
    "print(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35134a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# extracting features\n",
    "def extract_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    # Punctuation counts \n",
    "    features['exclamation_count'] = df['text'].str.count('!')\n",
    "    features['question_count'] = df['text'].str.count('\\?')\n",
    "    features['ellipsis_count'] = df['text'].str.count(r'\\.\\.\\.')\n",
    "    \n",
    "    # Capitalization\n",
    "    features['capital_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n",
    "    features['has_all_caps_word'] = df['text'].str.contains(r'\\b[A-Z]{2,}\\b').astype(int)\n",
    "    \n",
    "    # Length metrics\n",
    "    features['text_length'] = df['text'].str.len()\n",
    "    features['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    return features\n",
    "\n",
    "train_features = extract_features(train_df)\n",
    "valid_features = extract_features(valid_df)\n",
    "test_features = extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ceca22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove all capitalization and whitespace\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # Clean whitespace\n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(preprocess_text)\n",
    "valid_df['clean_text'] = valid_df['text'].apply(preprocess_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0496ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_clean = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf_clean = tfidf_clean.fit_transform(train_df['clean_text'])\n",
    "X_valid_tfidf_clean = tfidf_clean.transform(valid_df['clean_text'])\n",
    "X_test_tfidf_clean = tfidf_clean.transform(test_df['clean_text'])\n",
    "\n",
    "X_train_combined = hstack([X_train_tfidf_clean, train_features])\n",
    "X_valid_combined = hstack([X_valid_tfidf_clean, valid_features])\n",
    "X_test_combined = hstack([X_test_tfidf_clean, test_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e3ea2",
   "metadata": {},
   "source": [
    "## Baseline Model (Log Regression w/ Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8d2f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy: 0.7598\n",
      "Baseline Validation F1 Score: 0.7485\n"
     ]
    }
   ],
   "source": [
    "# we limit to the top 5000 most frequent words to keep it simple\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# fit on training data, then transform valid and test data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_valid = vectorizer.transform(valid_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# log reg model\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluation\n",
    "valid_preds = baseline_model.predict(X_valid)\n",
    "\n",
    "print(f\"Baseline Validation Accuracy: {accuracy_score(y_valid, valid_preds):.4f}\")\n",
    "print(f\"Baseline Validation F1 Score: {f1_score(y_valid, valid_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973c201",
   "metadata": {},
   "source": [
    "Feature Engineering with TF-IDF and N-grams\n",
    "\n",
    "Idea: weight words to give less importance to common words and more importance to unique words\n",
    "\n",
    "-> This might signal sarcasm?\n",
    "\n",
    "N-gram Idea: model seeing pairs of words together might be important for sarcasm b/c gives more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0aa649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Bigram Accuracy: 0.7709\n",
      "TF-IDF + Bigram F1 Score: 0.7595\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(valid_df['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# use new features to retrain\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "tfidf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# evaluation\n",
    "valid_preds_tfidf = tfidf_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"TF-IDF + Bigram Accuracy: {accuracy_score(y_valid, valid_preds_tfidf):.4f}\")\n",
    "print(f\"TF-IDF + Bigram F1 Score: {f1_score(y_valid, valid_preds_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb4b9e",
   "metadata": {},
   "source": [
    "## Possible model (SVM)\n",
    "\n",
    "Idea: Good at classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98f26cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.7751\n",
      "SVM Validation F1 Score: 0.7636\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC is faster and often better for text than standard SVC\n",
    "svm_model = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# evaluation\n",
    "valid_preds_svm = svm_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"SVM Validation Accuracy: {accuracy_score(y_valid, valid_preds_svm):.4f}\")\n",
    "print(f\"SVM Validation F1 Score: {f1_score(y_valid, valid_preds_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce1025",
   "metadata": {},
   "source": [
    "## Possible model (LSTM)\n",
    "\n",
    "LSTM: process text as a sequence rather than Bag of Words\n",
    "\n",
    "Idea: capture more structure in the sarcasm string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b06d5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fieryhacker/sarml/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.7343 - loss: 0.5256 - val_accuracy: 0.8464 - val_loss: 0.3561\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.8927 - loss: 0.2621 - val_accuracy: 0.8464 - val_loss: 0.3433\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.9367 - loss: 0.1702 - val_accuracy: 0.8478 - val_loss: 0.3913\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 31ms/step - accuracy: 0.9613 - loss: 0.1129 - val_accuracy: 0.8422 - val_loss: 0.4913\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 32ms/step - accuracy: 0.9750 - loss: 0.0741 - val_accuracy: 0.8240 - val_loss: 0.5673\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "LSTM Validation Accuracy: 0.8240\n",
      "LSTM Validation F1 Score: 0.8147\n"
     ]
    }
   ],
   "source": [
    "# we limit the vocab to 10,000 words and sequence length to 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=100)\n",
    "X_valid_seq = pad_sequences(tokenizer.texts_to_sequences(valid_df['text']), maxlen=100)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=100)\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# evaluation\n",
    "lstm_probs = model.predict(X_valid_seq)\n",
    "lstm_preds = (lstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Validation Accuracy: {accuracy_score(y_valid, lstm_preds):.4f}\")\n",
    "print(f\"LSTM Validation F1 Score: {f1_score(y_valid, lstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79784e34",
   "metadata": {},
   "source": [
    "## Possible model (BiLSTM)\n",
    "\n",
    "BiLSTM to read strings bidirectionally\n",
    "\n",
    "Idea: typically better to gain more information.\n",
    "\n",
    "Optimization: Dropout so that the network is not overly reliant on specific features (prevents overfitting)\n",
    "\n",
    "UPDATE: added stacking and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f28203e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fieryhacker/sarml/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 107ms/step - accuracy: 0.7880 - loss: 0.4417 - val_accuracy: 0.8534 - val_loss: 0.3527\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 115ms/step - accuracy: 0.9171 - loss: 0.2333 - val_accuracy: 0.8380 - val_loss: 0.3864\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 116ms/step - accuracy: 0.9535 - loss: 0.1366 - val_accuracy: 0.8296 - val_loss: 0.4778\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 123ms/step - accuracy: 0.9729 - loss: 0.0879 - val_accuracy: 0.8324 - val_loss: 0.6297\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step\n",
      "Bi-LSTM Accuracy: 0.8534\n",
      "Bi-LSTM F1 Score: 0.8502\n"
     ]
    }
   ],
   "source": [
    "# we re-fit the tokenizer on the cleaner text\n",
    "tokenizer_clean = Tokenizer(num_words=10000)\n",
    "tokenizer_clean.fit_on_texts(train_df['clean_text'])\n",
    "\n",
    "X_train_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(train_df['clean_text']), maxlen=100)\n",
    "X_valid_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(valid_df['clean_text']), maxlen=100)\n",
    "X_test_seq_clean = pad_sequences(tokenizer_clean.texts_to_sequences(test_df['clean_text']), maxlen=100)\n",
    "\n",
    "# stack two Bidirectional LSTM layers to learn more complex patterns\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# stop training if validation loss doesn't improve for 3 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "bilstm_model.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# evaluation\n",
    "probs_bilstm_improved = bilstm_model.predict(X_valid_seq_clean)\n",
    "bilstm_preds = (probs_bilstm_improved > 0.5).astype(int)\n",
    "\n",
    "print(f\"Bi-LSTM Accuracy: {accuracy_score(y_valid, bilstm_preds):.4f}\")\n",
    "print(f\"Bi-LSTM F1 Score: {f1_score(y_valid, bilstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f57a1",
   "metadata": {},
   "source": [
    "## Possible Model (CNN)\n",
    "\n",
    "CNN for text classification to spot N-gram patterns\n",
    "\n",
    "Idea: detect phrases for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12e2d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fieryhacker/sarml/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 29ms/step - accuracy: 0.7943 - loss: 0.4406 - val_accuracy: 0.8575 - val_loss: 0.3460\n",
      "Epoch 2/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.9162 - loss: 0.2273 - val_accuracy: 0.8436 - val_loss: 0.3687\n",
      "Epoch 3/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.9687 - loss: 0.1061 - val_accuracy: 0.8366 - val_loss: 0.4920\n",
      "Epoch 4/10\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.9881 - loss: 0.0483 - val_accuracy: 0.8436 - val_loss: 0.6216\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "CNN Validation Accuracy: 0.8575\n",
      "CNN Validation F1 Score: 0.8579\n"
     ]
    }
   ],
   "source": [
    "# CNN model\n",
    "cnn_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(\n",
    "    X_train_seq_clean, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_seq_clean, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# evaluation\n",
    "probs_cnn = cnn_model.predict(X_valid_seq_clean)\n",
    "preds_cnn = (probs_cnn > 0.5).astype(int)\n",
    "\n",
    "print(f\"CNN Validation Accuracy: {accuracy_score(y_valid, preds_cnn):.4f}\")\n",
    "print(f\"CNN Validation F1 Score: {f1_score(y_valid, preds_cnn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d922d2",
   "metadata": {},
   "source": [
    "## Inital Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3696d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy: 0.8450\n",
      "Ensemble Validation F1 Score: 0.8380\n"
     ]
    }
   ],
   "source": [
    "pred_1 = valid_preds_svm\n",
    "pred_2 = lstm_preds.flatten()\n",
    "pred_3 = bilstm_preds.flatten()\n",
    "\n",
    "total_votes = pred_1 + pred_2 + pred_3\n",
    "ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "print(f\"Ensemble Validation Accuracy: {accuracy_score(y_valid, ensemble_preds):.4f}\")\n",
    "print(f\"Ensemble Validation F1 Score: {f1_score(y_valid, ensemble_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9f4fb",
   "metadata": {},
   "source": [
    "Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010c3a5",
   "metadata": {},
   "source": [
    "## Testing different Ensemble methods and combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "431010af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Final Evaluation on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8609    0.8821    0.8714       526\n",
      "           1     0.8548    0.8295    0.8420       440\n",
      "\n",
      "    accuracy                         0.8582       966\n",
      "   macro avg     0.8578    0.8558    0.8567       966\n",
      "weighted avg     0.8581    0.8582    0.8580       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[464  62]\n",
      " [ 75 365]]\n"
     ]
    }
   ],
   "source": [
    "# Ensemble method: SVM -> LSTM -> BiLSTM w/ Ensemble Voting\n",
    "pred_test_svm = svm_model.predict(X_test_tfidf)\n",
    "pred_test_lstm = (model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "pred_test_bilstm = (bilstm_model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "test_votes = pred_test_svm + pred_test_lstm + pred_test_bilstm\n",
    "pred_test_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# evaluation\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "print(classification_report(test_df['label'], pred_test_ensemble, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], pred_test_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "304e6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "SUMMARY:\n",
      " - Accuracy: 0.8561\n",
      " - F1 Score: 0.8404\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8617    0.8764    0.8690       526\n",
      "           1     0.8492    0.8318    0.8404       440\n",
      "\n",
      "    accuracy                         0.8561       966\n",
      "   macro avg     0.8554    0.8541    0.8547       966\n",
      "weighted avg     0.8560    0.8561    0.8560       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[461  65]\n",
      " [ 74 366]]\n"
     ]
    }
   ],
   "source": [
    "# Ensemble method: SVM -> BiLSTM -> CNN w/ Voting\n",
    "test_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "test_probs_bilstm = bilstm_model.predict(X_test_seq_clean)\n",
    "test_pred_bilstm = (test_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# voting\n",
    "test_votes = test_pred_svm + test_pred_bilstm + test_pred_cnn\n",
    "final_ensemble_preds = (test_votes >= 2).astype(int)\n",
    "\n",
    "# evaluation\n",
    "print(\"SUMMARY:\")\n",
    "print(f\" - Accuracy: {accuracy_score(test_df['label'], final_ensemble_preds):.4f}\")\n",
    "print(f\" - F1 Score: {f1_score(test_df['label'], final_ensemble_preds):.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(test_df['label'], final_ensemble_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], final_ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f55a8ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
      "SUMMARY:\n",
      " - Accuracy: 0.8602\n",
      " - F1 Score: 0.8443\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8627    0.8840    0.8732       526\n",
      "           1     0.8571    0.8318    0.8443       440\n",
      "\n",
      "    accuracy                         0.8602       966\n",
      "   macro avg     0.8599    0.8579    0.8588       966\n",
      "weighted avg     0.8602    0.8602    0.8601       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[465  61]\n",
      " [ 74 366]]\n"
     ]
    }
   ],
   "source": [
    "# Ensemble method: SVM -> LSTM -> BiLSTM w/ Voting\n",
    "pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "pred_probs_lstm = model.predict(X_test_seq) \n",
    "pred_lstm = (pred_probs_lstm > 0.5).astype(int).flatten()\n",
    "\n",
    "pred_probs_bilstm = bilstm_model.predict(X_test_seq_clean)\n",
    "pred_bilstm = (pred_probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "# voting\n",
    "combined_votes = pred_svm + pred_lstm + pred_bilstm\n",
    "ensemble_final_preds = (combined_votes >= 2).astype(int)\n",
    "\n",
    "# evaluation\n",
    "\n",
    "print(\"SUMMARY:\")\n",
    "print(f\" - Accuracy: {accuracy_score(test_df['label'], ensemble_final_preds):.4f}\")\n",
    "print(f\" - F1 Score: {f1_score(test_df['label'], ensemble_final_preds):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], ensemble_final_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], ensemble_final_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cedb2657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "SUMMARY: \n",
      " - Accuracy: 0.8571\n",
      " - F1 Score: 0.8417\n",
      "\n",
      "Detailed Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8633    0.8764    0.8698       526\n",
      "           1     0.8495    0.8341    0.8417       440\n",
      "\n",
      "    accuracy                         0.8571       966\n",
      "   macro avg     0.8564    0.8553    0.8558       966\n",
      "weighted avg     0.8570    0.8571    0.8570       966\n",
      "\n",
      "\n",
      "Confusion Matrix (Actual vs Predicted):\n",
      "[[461  65]\n",
      " [ 73 367]]\n"
     ]
    }
   ],
   "source": [
    "# Ensemble method: SVM -> LSTM -> CNN w/ Voting\n",
    "test_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "test_probs_lstm = model.predict(X_test_seq)\n",
    "test_pred_lstm = (test_probs_lstm > 0.5).astype(int).flatten()\n",
    "\n",
    "test_probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "test_pred_cnn = (test_probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# voting\n",
    "total_votes = test_pred_svm + test_pred_lstm + test_pred_cnn\n",
    "hybrid_ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# evaluation\n",
    "\n",
    "print(\"SUMMARY: \")\n",
    "print(f\" - Accuracy: {accuracy_score(test_df['label'], hybrid_ensemble_preds):.4f}\")\n",
    "print(f\" - F1 Score: {f1_score(test_df['label'], hybrid_ensemble_preds):.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Performance:\")\n",
    "print(classification_report(test_df['label'], hybrid_ensemble_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Actual vs Predicted):\")\n",
    "print(confusion_matrix(test_df['label'], hybrid_ensemble_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d324303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "SUMMARY:\n",
      " - Accuracy: 0.8613\n",
      " - F1 Score: 0.8477\n",
      "\n",
      "Detailed Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8726    0.8726    0.8726       526\n",
      "           1     0.8477    0.8477    0.8477       440\n",
      "\n",
      "    accuracy                         0.8613       966\n",
      "   macro avg     0.8602    0.8602    0.8602       966\n",
      "weighted avg     0.8613    0.8613    0.8613       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[459  67]\n",
      " [ 67 373]]\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Method: LSTM -> BiLSTM -> CNN\n",
    "probs_lstm = model.predict(X_test_seq)\n",
    "pred_lstm = (probs_lstm > 0.5).astype(int).flatten()\n",
    "\n",
    "probs_bilstm = bilstm_model.predict(X_test_seq_clean)\n",
    "pred_bilstm = (probs_bilstm > 0.5).astype(int).flatten()\n",
    "\n",
    "probs_cnn = cnn_model.predict(X_test_seq_clean)\n",
    "pred_cnn = (probs_cnn > 0.5).astype(int).flatten()\n",
    "\n",
    "# voting\n",
    "total_votes = pred_lstm + pred_bilstm + pred_cnn\n",
    "deep_ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# evaluation\n",
    "print(\"SUMMARY:\")\n",
    "print(f\" - Accuracy: {accuracy_score(test_df['label'], deep_ensemble_preds):.4f}\")\n",
    "print(f\" - F1 Score: {f1_score(test_df['label'], deep_ensemble_preds):.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(classification_report(test_df['label'], deep_ensemble_preds, digits=4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], deep_ensemble_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041d669",
   "metadata": {},
   "source": [
    "## LSTM -> BiLSTM -> CNN is the best Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc12c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model.save('lstm_model.h5')\n",
    "bilstm_model.save('bilstm_model.h5')\n",
    "cnn_model.save('cnn_model.h5')\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('tokenizer_clean.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer_clean, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
