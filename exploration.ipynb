{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f7141c",
   "metadata": {},
   "source": [
    "Exploration of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a58bcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (21464, 2)\n",
      "Valid Shape: (716, 2)\n",
      "Test Shape: (966, 2)\n",
      "\n",
      "                                                text  label\n",
      "0  states slow to shut down weak teacher educatio...      0\n",
      "1    drone places fresh kill on steps of white house      1\n",
      "2  report: majority of instances of people gettin...      1\n",
      "3  sole remaining lung filled with rich, satisfyi...      1\n",
      "4                       the gop's stockholm syndrome      0\n",
      "\n",
      "label\n",
      "0    11248\n",
      "1    10216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('./datasets/train.csv')\n",
    "valid_df = pd.read_csv('./datasets/valid.csv')\n",
    "test_df = pd.read_csv('./datasets/test.csv')\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"Train Shape: {train_df.shape}\")\n",
    "print(f\"Valid Shape: {valid_df.shape}\")\n",
    "print(f\"Test Shape: {test_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Preview the training data\n",
    "print(train_df.head())\n",
    "print()\n",
    "\n",
    "# Check for class balance in the training set\n",
    "print(train_df['label'].value_counts())\n",
    "print()\n",
    "\n",
    "# Check for any missing values\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e3ea2",
   "metadata": {},
   "source": [
    "Baseline Model with Bag of Words and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d2f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Validation Accuracy: 0.7570\n",
      "Baseline Validation F1 Score: 0.7464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1. Preprocessing: Convert text to numerical vectors (Bag of Words)\n",
    "# We limit to the top 5000 most frequent words to keep it simple\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit on training data, then transform valid and test data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "X_valid = vectorizer.transform(valid_df['text'])\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# 2. Model: Train a simple Logistic Regression model\n",
    "baseline_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluation: specific metrics on validation set\n",
    "valid_preds = baseline_model.predict(X_valid)\n",
    "\n",
    "print(f\"Baseline Validation Accuracy: {accuracy_score(y_valid, valid_preds):.4f}\")\n",
    "print(f\"Baseline Validation F1 Score: {f1_score(y_valid, valid_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973c201",
   "metadata": {},
   "source": [
    "Feature Engineering with TF-IDF and N-grams\n",
    "\n",
    "Idea: weight words to give less importance to common words and more importance to unique words\n",
    "\n",
    "-> This might signal sarcasm?\n",
    "\n",
    "N-gram Idea: model seeing pairs of words together might be important for sarcasm b/c gives more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0aa649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Bigram Accuracy: 0.7737\n",
      "TF-IDF + Bigram F1 Score: 0.7632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Preprocessing: Use TF-IDF and include Bigrams (1-word and 2-word combinations)\n",
    "# We increase max_features slightly to accommodate new bigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(valid_df['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# 2. Model: Retrain Logistic Regression on these new features\n",
    "tfidf_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "tfidf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Evaluation\n",
    "valid_preds_tfidf = tfidf_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"TF-IDF + Bigram Accuracy: {accuracy_score(y_valid, valid_preds_tfidf):.4f}\")\n",
    "print(f\"TF-IDF + Bigram F1 Score: {f1_score(y_valid, valid_preds_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb4b9e",
   "metadata": {},
   "source": [
    "Support Vector Machine\n",
    "Idea: Good at classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f26cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.7835\n",
      "SVM Validation F1 Score: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1. Model: Support Vector Machine (Linear Kernel)\n",
    "# LinearSVC is faster and often better for text than standard SVC\n",
    "svm_model = LinearSVC(random_state=42, max_iter=10000)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Evaluation\n",
    "valid_preds_svm = svm_model.predict(X_valid_tfidf)\n",
    "\n",
    "print(f\"SVM Validation Accuracy: {accuracy_score(y_valid, valid_preds_svm):.4f}\")\n",
    "print(f\"SVM Validation F1 Score: {f1_score(y_valid, valid_preds_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce1025",
   "metadata": {},
   "source": [
    "LSTM: process text as a sequence rather than Bag of Words\n",
    "\n",
    "Idea: capture more structure in the sarcasm string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06d5407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.7632 - loss: 0.4733 - val_accuracy: 0.8743 - val_loss: 0.3368\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9023 - loss: 0.2423 - val_accuracy: 0.8478 - val_loss: 0.3528\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9385 - loss: 0.1676 - val_accuracy: 0.8534 - val_loss: 0.4057\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9565 - loss: 0.1206 - val_accuracy: 0.8380 - val_loss: 0.4715\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9726 - loss: 0.0808 - val_accuracy: 0.8450 - val_loss: 0.6123\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "LSTM Validation Accuracy: 0.8450\n",
      "LSTM Validation F1 Score: 0.8384\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# 1. Preprocessing: Convert text to sequences of integers\n",
    "# We limit the vocab to 10,000 words and sequence length to 100\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=100)\n",
    "X_valid_seq = pad_sequences(tokenizer.texts_to_sequences(valid_df['text']), maxlen=100)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=100)\n",
    "\n",
    "# 2. Model: Define a simple LSTM network \n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Training\n",
    "model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 4. Evaluation\n",
    "lstm_probs = model.predict(X_valid_seq)\n",
    "lstm_preds = (lstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"LSTM Validation Accuracy: {accuracy_score(y_valid, lstm_preds):.4f}\")\n",
    "print(f\"LSTM Validation F1 Score: {f1_score(y_valid, lstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79784e34",
   "metadata": {},
   "source": [
    "BiLSTM to read strings bidirectionally\n",
    "\n",
    "Idea: typically better to gain more information.\n",
    "\n",
    "Optimization: Dropout so that the network is not overly reliant on specific features (prevents overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28203e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylanwong/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.7233 - loss: 0.5235 - val_accuracy: 0.8408 - val_loss: 0.3720\n",
      "Epoch 2/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - accuracy: 0.8880 - loss: 0.2764 - val_accuracy: 0.8492 - val_loss: 0.3457\n",
      "Epoch 3/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.9311 - loss: 0.1879 - val_accuracy: 0.8450 - val_loss: 0.3854\n",
      "Epoch 4/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.9520 - loss: 0.1365 - val_accuracy: 0.8436 - val_loss: 0.4668\n",
      "Epoch 5/5\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 36ms/step - accuracy: 0.9664 - loss: 0.0971 - val_accuracy: 0.8380 - val_loss: 0.5728\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Bi-LSTM Validation Accuracy: 0.8380\n",
      "Bi-LSTM Validation F1 Score: 0.8371\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "\n",
    "# 1. Model: Define a Bidirectional LSTM with Dropout\n",
    "# We wrap the LSTM layer in 'Bidirectional'\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=32, input_length=100),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 2. Training\n",
    "# We use the same sequence data (X_train_seq) prepared in the previous step\n",
    "bilstm_model.fit(X_train_seq, y_train, epochs=5, batch_size=64, validation_data=(X_valid_seq, y_valid))\n",
    "\n",
    "# 3. Evaluation\n",
    "bilstm_probs = bilstm_model.predict(X_valid_seq)\n",
    "bilstm_preds = (bilstm_probs > 0.5).astype(int)\n",
    "\n",
    "print(f\"Bi-LSTM Validation Accuracy: {accuracy_score(y_valid, bilstm_preds):.4f}\")\n",
    "print(f\"Bi-LSTM Validation F1 Score: {f1_score(y_valid, bilstm_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ca957",
   "metadata": {},
   "source": [
    "Why did the accuracy go down bro :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d922d2",
   "metadata": {},
   "source": [
    "Build Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3696d397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy: 0.8408\n",
      "Ensemble Validation F1 Score: 0.8376\n"
     ]
    }
   ],
   "source": [
    "# 1. Gather predictions from our top 3 models\n",
    "# Note: We flatten the neural network arrays to make them match the SVM's shape\n",
    "pred_1 = valid_preds_svm\n",
    "pred_2 = lstm_preds.flatten()\n",
    "pred_3 = bilstm_preds.flatten()\n",
    "\n",
    "# 2. Voting Logic: Sum the predictions\n",
    "# If sum is 2 or 3, it means the majority voted 1 (Sarcasm)\n",
    "total_votes = pred_1 + pred_2 + pred_3\n",
    "ensemble_preds = (total_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Evaluation\n",
    "print(f\"Ensemble Validation Accuracy: {accuracy_score(y_valid, ensemble_preds):.4f}\")\n",
    "print(f\"Ensemble Validation F1 Score: {f1_score(y_valid, ensemble_preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9f4fb",
   "metadata": {},
   "source": [
    "Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f454a06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Final Evaluation on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8707    0.8707    0.8707       526\n",
      "           1     0.8455    0.8455    0.8455       440\n",
      "\n",
      "    accuracy                         0.8592       966\n",
      "   macro avg     0.8581    0.8581    0.8581       966\n",
      "weighted avg     0.8592    0.8592    0.8592       966\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[458  68]\n",
      " [ 68 372]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Get predictions for the Test Set from all 3 models\n",
    "# SVM (uses TF-IDF features)\n",
    "pred_test_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# LSTM (uses Sequence features)\n",
    "pred_test_lstm = (model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Bi-LSTM (uses Sequence features)\n",
    "pred_test_bilstm = (bilstm_model.predict(X_test_seq) > 0.5).astype(int).flatten()\n",
    "\n",
    "# 2. Ensemble Voting (Majority Vote)\n",
    "# Sum the predictions (0 or 1). If sum is 2 or 3, majority is 1.\n",
    "test_votes = pred_test_svm + pred_test_lstm + pred_test_bilstm\n",
    "pred_test_ensemble = (test_votes >= 2).astype(int)\n",
    "\n",
    "# 3. Report detailed metrics\n",
    "print(\"Final Evaluation on Test Set:\")\n",
    "print(classification_report(test_df['label'], pred_test_ensemble, digits=4))\n",
    "\n",
    "# 4. Confusion Matrix (Row: True, Col: Predicted)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_df['label'], pred_test_ensemble))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
